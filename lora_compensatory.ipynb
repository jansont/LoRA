{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import wandb\n",
    "import pickle\n",
    "import json\n",
    "import types\n",
    "import torch\n",
    "import random\n",
    "import argparse\n",
    "import warnings\n",
    "import itertools\n",
    "import warnings\n",
    "import loralib as lora\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from transformer_lens import HookedTransformer\n",
    "from gpt2_lora.data_utils import FT_Dataset\n",
    "from gpt2_lora.model import GPT2LMModel, GPT2Config, LORAConfig\n",
    "from gpt2_lora.training.train import train_validate\n",
    "from gpt2_lora.correction_dataset import CorrectionDataset, create_lm_dataset \n",
    "import gpt2_lora.ablations as ablations\n",
    "import gpt2_lora.activation_graft as activation_grafts\n",
    "from gpt2_lora.training.optimizer import (\n",
    "    create_optimizer_scheduler, \n",
    "    add_optimizer_params, \n",
    "    create_adam_optimizer_from_args\n",
    ")\n",
    "from gpt2_lora.exp_utils import create_exp_dir\n",
    "from gpt2_lora.training.evaluate import evaluate\n",
    "from gpt2_lora.utils import set_all_trainable, set_trainable_from_graft, AverageMeter, log_experiment\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lora_compensation.hooking import get_residuals_and_logits\n",
    "from timeout_decorator import timeout, TimeoutError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_args(args):\n",
    "    if args.rank == 0:\n",
    "        print('=' * 100)\n",
    "        for k, v in args.__dict__.items():\n",
    "            print(f'        - {k} : {v}')\n",
    "        print('=' * 100)\n",
    "        \n",
    "def get_device():\n",
    "    return \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def validate_args(args):\n",
    "    if args.task not in ['lora_graft_finetune', 'lora_mlp_finetune', 'lora_attn_finetune', 'lora_all_finetune', 'finetune', 'graft_finetune']: \n",
    "        raise ValueError(\"task not recognized\")\n",
    "    if args.task==\"lora_graft_finetune\": \n",
    "        if sum([args.adapt_mlp_c_fc, args.adapt_mlp_c_proj, args.adapt_attn_c_attn, args.adapt_attn_c_proj]) == 0: \n",
    "            raise ValueError(\"No LoRA layers selected\")\n",
    "    if args.task==\"lora_mlp_finetune\": \n",
    "        if sum([args.adapt_mlp_c_fc, args.adapt_mlp_c_proj]) == 0: \n",
    "            raise ValueError(\"No LoRA MLP layers selected\")\n",
    "    if args.task==\"lora_attn_finetune\": \n",
    "        if sum([args.aadapt_attn_c_attn, args.adapt_attn_c_proj]) == 0: \n",
    "            raise ValueError(\"No LoRA Attention layers selected\")\n",
    "    if args.graft_type not in [\"decomposition\", \"causal_total_effect\", \"causal_total_effect_window\", \"causal_direct_effect_window\"]: \n",
    "        raise ValueError(\"graft_type not recognized\")\n",
    "    if args.ablation_method not in [\"noise\", \"resample\", \"resample_uniform\"]: \n",
    "        raise ValueError(\"ablation_method not recognized\")\n",
    "    \n",
    "        \n",
    "def parse_args(config_path=\"configs/config_lora_compensatory.yaml\"):\n",
    "    # Load YAML configuration\n",
    "    with open(config_path, 'r') as config_file:\n",
    "        config = yaml.safe_load(config_file)\n",
    "    \n",
    "    # Create a custom namespace object\n",
    "    args = types.SimpleNamespace()\n",
    "    \n",
    "    # Set the configuration values as attributes of args\n",
    "    for key, value in config.items():\n",
    "        setattr(args, key, value)\n",
    "\n",
    "    # Set additional attributes or perform any other processing as needed\n",
    "    setattr(args, 'device', get_device())  # Example: Set the 'device' attribute\n",
    "    \n",
    "    # Validate args or print them if necessary\n",
    "    validate_args(args)\n",
    "    print_args(args)\n",
    "    \n",
    "    return args\n",
    "\n",
    "def generate_lora_configs(layer: int, n_layers: int, args : types.SimpleNamespace, adapt_attn=True):\n",
    "    lora_configs = [\n",
    "        {\"attn\" : None, \"mlp\" : None} for _ in range(n_layers)\n",
    "    ]\n",
    "    if layer is None:\n",
    "        return lora_configs\n",
    "    if adapt_attn:\n",
    "        lora_configs[layer][\"attn\"] = LORAConfig(\n",
    "                        layer=layer,\n",
    "                        layer_type=\"attn\",\n",
    "                        adapt_attn_c_attn=args.adapt_attn_c_attn,\n",
    "                        adapt_attn_c_proj=args.adapt_attn_c_proj,\n",
    "                        adapt_mlp_c_fc=False,\n",
    "                        adapt_mlp_c_proj=False,\n",
    "                        lora_dim=args.lora_dim,\n",
    "                        lora_alpha=args.lora_alpha,\n",
    "                        lora_dropout=args.lora_dropout)\n",
    "    else: \n",
    "        lora_configs[layer][\"mlp\"] = LORAConfig(\n",
    "                        layer=layer,\n",
    "                        layer_type=\"mlp\",\n",
    "                        adapt_attn_c_attn=False,\n",
    "                        adapt_attn_c_proj=False,\n",
    "                        adapt_mlp_c_fc=args.adapt_mlp_c_fc,\n",
    "                        adapt_mlp_c_proj=args.adapt_mlp_c_proj,\n",
    "                        lora_dim=args.lora_dim,\n",
    "                        lora_alpha=args.lora_alpha,\n",
    "                        lora_dropout=args.lora_dropout) \n",
    "    return lora_configs\n",
    "\n",
    "\n",
    "def save_pickle(obj, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(obj, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_control(args): \n",
    "    \n",
    "    if args.model_name == \"gpt2-small\":\n",
    "            hf_model_name = \"gpt2\"\n",
    "            n_layer = 12\n",
    "            config = GPT2Config(\n",
    "                n_embd=768, n_layer=n_layer, n_head=12, \n",
    "    )\n",
    "    elif args.model_name == \"gpt2-large\":\n",
    "            hf_model_name = args.model_name\n",
    "            n_layer = 36\n",
    "            config = GPT2Config(\n",
    "                n_embd=1280, n_layer=n_layer, n_head=20, \n",
    "    )\n",
    "    else: \n",
    "        raise ValueError(\"model_name not recognized\")\n",
    "    lora_configs = generate_lora_configs(None, n_layer, args)\n",
    "\n",
    "    lm_net = GPT2LMModel(config, lora_configs)\n",
    "    \n",
    "    model = GPT2LMHeadModel.from_pretrained(hf_model_name)\n",
    "    state_dict = model.state_dict()\n",
    "    lm_net.load_weight(state_dict)   \n",
    "    \n",
    "    model = HookedTransformer.from_pretrained(\n",
    "        args.model_name, \n",
    "        hf_model=lm_net,\n",
    "        lora_case=True\n",
    "    )\n",
    "    correction_dataset = CorrectionDataset(args.fact_data)\n",
    "    \n",
    "    #start by getting the base results\n",
    "    correction_dataloader = DataLoader(correction_dataset, batch_size=1)\n",
    "    initial_results = []\n",
    "    print(\"..getting initial results\")\n",
    "    for batch_idx, batch in enumerate(correction_dataloader):\n",
    "        #----------------------------Prepare Correction Dataset-----------------------------#\n",
    "        prompt = batch[\"prompt\"][0]\n",
    "        subject = batch[\"subject\"][0]\n",
    "        target = batch[\"target\"][0]\n",
    "        target_new = batch[\"target_new\"][0]\n",
    "        training_prompts = [p[0] for p in batch[\"training_prompts\"]]\n",
    "        \n",
    "        @timeout(30)\n",
    "        def timeout_resample(ablation_method):\n",
    "            if ablation_method == \"resample_uniform\": \n",
    "                original_fact, corrupted_facts, _ = ablations.resample_ablation_uniform(model, prompt,subject,target,                                                             n_noise_samples=args.noise_samples)\n",
    "            elif ablation_method==\"resample\":\n",
    "                original_fact, corrupted_facts, _ = ablations.resample_ablation(model, prompt, subject, target, n_noise_samples=args.noise_samples, temperature=args.temperature)\n",
    "            elif ablation_method==\"noise\": \n",
    "                original_fact, corrupted_facts, _ = ablations.noise_ablation(model, prompt,subject,target,n_noise_samples=args.noise_samples)\n",
    "            else: \n",
    "                raise ValueError(\"ablation_method not recognized\")\n",
    "            return original_fact, corrupted_facts\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            original_fact, corrupted_facts = timeout_resample(args.ablation_method)\n",
    "        except TimeoutError:\n",
    "            warnings.warn(f\"Resample timed out for prompt {prompt}\")\n",
    "            continue\n",
    "\n",
    "        \n",
    "        res = get_residuals_and_logits(model, \n",
    "                                args.device,\n",
    "                                clean_prompt=original_fact,\n",
    "                                corrupted_prompts=corrupted_facts,\n",
    "                                target=target, \n",
    "                                target_new=target_new, \n",
    "                                ablate_with_corrupted=True)\n",
    "        initial_results.append(res)\n",
    "        save_pickle(initial_results, f\"lora_compensation/results/{args.experiment_name}_initial_results.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parse_args()\n",
    "if args.do_wandb: \n",
    "    wandb.login()\n",
    "    \n",
    "random.seed(args.random_seed)\n",
    "torch.manual_seed(args.random_seed)   \n",
    "\n",
    "run_control(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(args): \n",
    "    if args.model_name == \"gpt2-small\":\n",
    "            hf_model_name = \"gpt2\"\n",
    "            n_layer = 12\n",
    "            config = GPT2Config(\n",
    "                n_embd=768, n_layer=n_layer, n_head=12, \n",
    "    )\n",
    "    elif args.model_name == \"gpt2-large\":\n",
    "            hf_model_name = args.model_name\n",
    "            n_layer = 36\n",
    "            config = GPT2Config(\n",
    "                n_embd=1280, n_layer=n_layer, n_head=20, \n",
    "    )\n",
    "    else: \n",
    "        raise ValueError(\"model_name not recognized\")\n",
    "    \n",
    "    correction_dataset = CorrectionDataset(args.fact_data)\n",
    "    \n",
    "    #start by getting the base results\n",
    "    correction_dataloader = DataLoader(correction_dataset, batch_size=1)\n",
    "    initial_results = []\n",
    "    print(\"..getting initial results\")\n",
    "    for batch_idx, batch in enumerate(correction_dataloader):\n",
    "        #----------------------------Prepare Correction Dataset-----------------------------#\n",
    "        prompt = batch[\"prompt\"][0]\n",
    "        subject = batch[\"subject\"][0]\n",
    "        target = batch[\"target\"][0]\n",
    "        target_new = batch[\"target_new\"][0]\n",
    "        training_prompts = [p[0] for p in batch[\"training_prompts\"]]\n",
    "        \n",
    "\n",
    "    for layer in range(n_layer):\n",
    "        if layer*2 % 2 == 0:\n",
    "            adapt_attention = True\n",
    "        else: \n",
    "            adapt_attention = False\n",
    "            \n",
    "        lora_configs = generate_lora_configs(layer, n_layer, args, adapt_attn=adapt_attention)\n",
    "            \n",
    "        lm_net = GPT2LMModel(config, lora_configs)\n",
    "        model = GPT2LMHeadModel.from_pretrained(hf_model_name)\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained(hf_model_name)\n",
    "        state_dict = model.state_dict()\n",
    "        lm_net.load_weight(state_dict)  \n",
    "        \n",
    "        \n",
    "        correction_dataset = CorrectionDataset(args.fact_data)\n",
    "        correction_dataloader = DataLoader(correction_dataset, batch_size=1)\n",
    "        \n",
    "        all_dataset = [] ; all_dataset_ref = []\n",
    "        for batch_idx, batch in enumerate(correction_dataloader):\n",
    "            #----------------------------Prepare Correction Dataset-----------------------------#\n",
    "            prompt = batch[\"prompt\"][0]\n",
    "            subject = batch[\"subject\"][0]\n",
    "            target = batch[\"target\"][0]\n",
    "            target_new = batch[\"target_new\"][0]\n",
    "            training_prompts = [p[0] for p in batch[\"training_prompts\"]]\n",
    "            \n",
    "            dataset = create_lm_dataset(\n",
    "                    prompts=all_dataset, target=target_new,\n",
    "                    subject=subject, tokenizer=tokenizer, args=args\n",
    "                )\n",
    "            dataset_ref = create_lm_dataset(\n",
    "                prompts=all_dataset_ref, target=target,\n",
    "                subject=subject, tokenizer=tokenizer, args=args\n",
    "            )\n",
    "            all_training_samples += dataset\n",
    "            all_training_samples_ref += dataset_ref\n",
    "            \n",
    "        dataset_indices = list(range(len(dataset)))\n",
    "        training_indices, valid_indices = train_test_split(\n",
    "            dataset_indices, test_size=args.test_size, random_state=args.random_seed\n",
    "        )\n",
    "        training_prompts = [d for i,d in enumerate(all_dataset) if i in training_indices]\n",
    "        valid_prompts = [d for i,d in enumerate(all_dataset) if i in valid_indices]\n",
    "        training_prompts_ref = [d for i,d in enumerate(all_dataset_ref) if i in training_indices]\n",
    "        valid_prompts_ref = [d for i,d in enumerate(all_dataset_ref) if i in valid_indices]\n",
    "        \n",
    "    \n",
    "        train_data = FT_Dataset(\n",
    "            samples=training_prompts,\n",
    "            ref_samples=training_prompts_ref,\n",
    "            batch_size=args.train_batch_size,\n",
    "            max_seq_length=args.seq_len, \n",
    "            joint_lm=args.obj=='jlm'\n",
    "        ) \n",
    "        valid_data = FT_Dataset(\n",
    "            samples=valid_prompts,\n",
    "            ref_samples=valid_prompts_ref,\n",
    "            batch_size=args.train_batch_size,\n",
    "            max_seq_length=args.seq_len, \n",
    "            joint_lm=args.obj=='jlm'\n",
    "        )     \n",
    "        train_loader = DataLoader(\n",
    "            train_data, batch_size=args.train_batch_size, num_workers=0, \n",
    "            shuffle=False, pin_memory=False, drop_last=True,\n",
    "        )\n",
    "        valid_loader = DataLoader(\n",
    "            valid_data, batch_size=args.valid_batch_size, num_workers=0, \n",
    "            shuffle=False, pin_memory=False, drop_last=False,\n",
    "        )\n",
    "        \n",
    "        #---------------------------------Training Model------------------------------------#\n",
    "        \n",
    "        if args.fp16:\n",
    "            try:\n",
    "                from torch.cuda import amp\n",
    "            except Exception as e:\n",
    "                warnings.warn('Could not import amp, apex may not be installed')\n",
    "        if args.max_step is None:\n",
    "            args.max_step = (args.max_epoch * train_data.num_batches) \n",
    "            print('set max_step:', args.max_step)\n",
    "        scheduler = create_optimizer_scheduler(optimizer, args)\n",
    "        if args.fp16:\n",
    "            lm_net, optimizer = amp.initialize(lm_net, optimizer, opt_level=\"O1\")\n",
    "        try:\n",
    "            train_step = 0\n",
    "            for epoch in itertools.count(start=1):\n",
    "                train_step = train_validate(\n",
    "                    lm_net, optimizer, scheduler, train_loader, valid_loader, args, \n",
    "                    train_step=train_step, epoch=epoch\n",
    "                )\n",
    "                print(\"REACH 5\")\n",
    "                \n",
    "                if train_step >= args.max_step or (args.max_epoch is not None and epoch >= args.max_epoch):\n",
    "                    if args.rank == 0:\n",
    "                        print('-' * 100)\n",
    "                        print('End of training')\n",
    "                    break\n",
    "        except KeyboardInterrupt:\n",
    "            if args.rank == 0:\n",
    "                print('-' * 100)\n",
    "                print('Exiting from training early')\n",
    "            early_exit = True\n",
    "            \n",
    "\n",
    "        #--------------------Hooking model------------------\n",
    "        model = HookedTransformer.from_pretrained(\n",
    "            args.model_name, \n",
    "            hf_model=lm_net,\n",
    "            lora_case=True\n",
    "        )\n",
    "        \n",
    "\n",
    "    correction_dataloader = DataLoader(correction_dataset, batch_size=1)\n",
    "    initial_results = []\n",
    "    for batch_idx, batch in enumerate(correction_dataloader):\n",
    "        #----------------------------Prepare Correction Dataset-----------------------------#\n",
    "        prompt = batch[\"prompt\"][0]\n",
    "        subject = batch[\"subject\"][0]\n",
    "        target = batch[\"target\"][0]\n",
    "        target_new = batch[\"target_new\"][0]\n",
    "        training_prompts = [p[0] for p in batch[\"training_prompts\"]]\n",
    "        \n",
    "        @timeout(30)\n",
    "        def timeout_resample(ablation_method):\n",
    "            if ablation_method == \"resample_uniform\": \n",
    "                original_fact, corrupted_facts, _ = ablations.resample_ablation_uniform(model, prompt,subject,target,                                                             n_noise_samples=args.noise_samples)\n",
    "            elif ablation_method==\"resample\":\n",
    "                original_fact, corrupted_facts, _ = ablations.resample_ablation(model, prompt, subject, target, n_noise_samples=args.noise_samples, temperature=args.temperature)\n",
    "            elif ablation_method==\"noise\": \n",
    "                original_fact, corrupted_facts, _ = ablations.noise_ablation(model, prompt,subject,target,n_noise_samples=args.noise_samples)\n",
    "            else: \n",
    "                raise ValueError(\"ablation_method not recognized\")\n",
    "            return original_fact, corrupted_facts\n",
    "        \n",
    "        try:\n",
    "            original_fact, corrupted_facts = timeout_resample(args.ablation_method)\n",
    "        except TimeoutError:\n",
    "            warnings.warn(f\"Resample timed out for prompt {prompt}\")\n",
    "            continue\n",
    "\n",
    "        res = get_residuals_and_logits(model, \n",
    "                                args.device,\n",
    "                                clean_prompt=original_fact,\n",
    "                                corrupted_prompts=corrupted_facts,\n",
    "                                target=target, \n",
    "                                target_new=target_new, \n",
    "                                ablate_with_corrupted=True)\n",
    "        initial_results.append(res)\n",
    "        module = \"attn\" if adapt_attention else \"mlp\"\n",
    "        save_pickle(initial_results, f\"lora_compensation/results/{args.experiment_name}_layer_{layer}_{module}_results.pkl\")\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parse_args()\n",
    "if args.do_wandb: \n",
    "    wandb.login()\n",
    "    \n",
    "random.seed(args.random_seed)\n",
    "torch.manual_seed(args.random_seed)   \n",
    "\n",
    "run_experiment(args)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
