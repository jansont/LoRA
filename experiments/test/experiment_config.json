{
    "config": "configs/config.yaml",
    "lr": 0.0001,
    "weight_decay": 0.01,
    "correct_bias": false,
    "adam_epislon": 1e-06,
    "no_decay_bias": false,
    "adam_beta1": 0.9,
    "adam_beta2": 0.98,
    "scheduler": "linear",
    "max_step": 100,
    "max_epoch": 50,
    "warmup_step": 0,
    "i_steps": "0",
    "i_lrs": "0.00025",
    "experiment_name": "test",
    "fact_data": "datasets/chatgpt_fact_dataset_test.json",
    "task": "lora_graft_finetune",
    "random_seed": 1,
    "model_name": "gpt2-large",
    "use_hf_model": true,
    "use_resample_ablation": true,
    "use_mle_token_graft": true,
    "use_causal_graft": false,
    "noise_samples": 10,
    "graft_threshold": 0.75,
    "do_wandb": false,
    "log_interval": 10,
    "eval_interval": 10,
    "save_interval": 500,
    "lora_dim": 2,
    "lora_alpha": 128,
    "lora_dropout": 0.2,
    "test_size": 0.1,
    "completion_size": 0.2,
    "train_batch_size": 8,
    "valid_batch_size": 4,
    "grad_acc": 1,
    "seq_len": 32,
    "init_checkpoint": null,
    "fp16": false,
    "work_dir": null,
    "obj": "clm",
    "label_smooth": 0.0,
    "roll_interval": -1,
    "roll_lr": 0.0001,
    "roll_step": 100,
    "eval_epoch": 1,
    "clip": 1.0,
    "rank": 0,
    "save_model": false,
    "device": "cpu",
    "logging": "functools.partial(<function logging at 0x7fbafbc60dc0>, log_path='gpt2_model/log.txt')"
}