{
    "config": "configs/config.yaml",
    "lr": 0.0001,
    "weight_decay": 0.01,
    "correct_bias": false,
    "adam_epislon": 1e-06,
    "no_decay_bias": false,
    "adam_beta1": 0.9,
    "adam_beta2": 0.98,
    "scheduler": "linear",
    "max_step": 80,
    "max_epoch": 5,
    "warmup_step": 0,
    "i_steps": "0",
    "i_lrs": "0.00025",
    "experiment_name": "decomposition_graft",
    "fact_data": "datasets/chatgpt_fact_dataset_100.json",
    "task": "lora_graft_finetune",
    "random_seed": 1,
    "model_name": "gpt2-small",
    "use_hf_model": true,
    "ablation_method": "resample_uniform",
    "use_mle_token_graft": false,
    "graft_type": "decomposition",
    "noise_samples": 10,
    "graft_threshold": 0.75,
    "temperature": 0.85,
    "window_size": 5,
    "window_stride": 1,
    "do_wandb": false,
    "log_interval": 10,
    "eval_interval": 10,
    "save_interval": 500,
    "lora_dim": 2,
    "lora_alpha": 128,
    "lora_dropout": 0.2,
    "adapt_mlp_c_fc": true,
    "adapt_mlp_c_proj": true,
    "adapt_attn_c_attn": true,
    "adapt_attn_c_proj": true,
    "test_size": 0.1,
    "completion_size": 0.2,
    "train_batch_size": 16,
    "valid_batch_size": 4,
    "grad_acc": 1,
    "seq_len": 32,
    "use_kl_reg": true,
    "init_checkpoint": null,
    "fp16": false,
    "work_dir": null,
    "obj": "clm",
    "label_smooth": 0.0,
    "roll_interval": -1,
    "roll_lr": 0.0001,
    "roll_step": 100,
    "eval_epoch": 1,
    "clip": 1.0,
    "rank": 0,
    "save_model": false,
    "device": "cpu",
    "logging": "functools.partial(<function logging at 0x7fc73d9f3010>, log_path='gpt2_model/log.txt')"
}