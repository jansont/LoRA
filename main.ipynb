{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import types\n",
    "import wandb\n",
    "import torch\n",
    "import random\n",
    "import argparse\n",
    "import warnings\n",
    "import itertools\n",
    "import warnings\n",
    "import loralib as lora\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from transformer_lens import HookedTransformer\n",
    "from gpt2_lora.data_utils import FT_Dataset\n",
    "from gpt2_lora.model import GPT2LMModel, GPT2Config\n",
    "from gpt2_lora.training.train import train_validate\n",
    "from gpt2_lora.correction_dataset import CorrectionDataset, create_lm_dataset\n",
    "import gpt2_lora.ablations as ablations\n",
    "import gpt2_lora.activation_graft as activation_grafts\n",
    "from gpt2_lora.training.optimizer import (\n",
    "    create_optimizer_scheduler, \n",
    "    add_optimizer_params, \n",
    "    create_adam_optimizer_from_args\n",
    ")\n",
    "from gpt2_lora.exp_utils import create_exp_dir\n",
    "from gpt2_lora.training.evaluate import evaluate\n",
    "from gpt2_lora.utils import set_all_trainable, set_trainable_from_graft, AverageMeter, log_experiment\n",
    "from sklearn.model_selection import train_test_split\n",
    "from timeout_decorator import timeout, TimeoutError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_args(args):\n",
    "    if args.rank == 0:\n",
    "        print('=' * 100)\n",
    "        for k, v in args.__dict__.items():\n",
    "            print(f'        - {k} : {v}')\n",
    "        print('=' * 100)\n",
    "        \n",
    "def get_device():\n",
    "    return \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def validate_args(args):\n",
    "    if args.task not in ['lora_graft_finetune', 'lora_mlp_finetune', 'lora_attn_finetune', 'lora_all_finetune', 'finetune', 'graft_finetune']: \n",
    "        raise ValueError(\"task not recognized\")\n",
    "    if args.task==\"lora_graft_finetune\": \n",
    "        if sum([args.adapt_mlp_c_fc, args.adapt_mlp_c_proj, args.adapt_attn_c_attn, args.adapt_attn_c_proj]) == 0: \n",
    "            raise ValueError(\"No LoRA layers selected\")\n",
    "    if args.task==\"lora_mlp_finetune\": \n",
    "        if sum([args.adapt_mlp_c_fc, args.adapt_mlp_c_proj]) == 0: \n",
    "            raise ValueError(\"No LoRA MLP layers selected\")\n",
    "    if args.task==\"lora_attn_finetune\": \n",
    "        if sum([args.aadapt_attn_c_attn, args.adapt_attn_c_proj]) == 0: \n",
    "            raise ValueError(\"No LoRA Attention layers selected\")\n",
    "    if args.graft_type not in [\"decomposition\", \"causal_total_effect\", \"causal_total_effect_window\", \"causal_direct_effect_window\"]: \n",
    "        raise ValueError(\"graft_type not recognized\")\n",
    "    if args.ablation_method not in [\"noise\", \"resample\", \"resample_uniform\"]: \n",
    "        raise ValueError(\"ablation_method not recognized\")\n",
    "    \n",
    "\n",
    "def parse_args(config_path=\"configs/config_lora_compensatory.yaml\"):\n",
    "    with open(config_path, 'r') as config_file:\n",
    "        config = yaml.safe_load(config_file)\n",
    "\n",
    "    args = types.SimpleNamespace()\n",
    "    \n",
    "    for key, value in config.items():\n",
    "        setattr(args, key, value)\n",
    "\n",
    "    setattr(args, 'device', get_device())      \n",
    "    validate_args(args)\n",
    "    print_args(args)\n",
    "    \n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prompt(batch, args): \n",
    "    early_exit = False\n",
    "    torch.set_grad_enabled(False)\n",
    "    hooked_model = HookedTransformer.from_pretrained(\n",
    "        args.model_name,\n",
    "        center_unembed=True,  \n",
    "        center_writing_weights=True,              # Whether to center weights writing to the residual stream (ie set mean to be zero). Due to LayerNorm this doesn't change the computation.      \n",
    "        fold_ln=True,                             # Whether to  fold in the LayerNorm weights to the subsequent linear layer.\n",
    "        refactor_factored_attn_matrices=True,\n",
    "    )\n",
    "    #----------------------------Prepare Correction Dataset-----------------------------#\n",
    "    prompt = batch[\"prompt\"][0]\n",
    "    subject = batch[\"subject\"][0]\n",
    "    target = batch[\"target\"][0]\n",
    "    target_new = batch[\"target_new\"][0]\n",
    "    training_prompts = [p[0] for p in batch[\"training_prompts\"]]\n",
    "    reference_evaluation_prompts = [p[0] for p in batch[\"reference_evaluation_prompts\"]]\n",
    "    neighborhood_prompts = [p[0] for p in batch[\"neighborhood_prompts\"]]\n",
    "    reference_neighborhood_prompts = [p[0] for p in batch[\"reference_neighborhood_prompts\"]]\n",
    "    same_attribute_prompts = [p[0] for p in batch[\"same_attribute_prompts\"]]\n",
    "    reference_same_attribute_prompts = [p[0] for p in batch[\"reference_same_attribute_prompts\"]]\n",
    "    \n",
    "    print(prompt)\n",
    "    print(subject)\n",
    "    print(target, target_new)\n",
    "    \n",
    "    @timeout(30)\n",
    "    def timeout_resample(ablation_method):\n",
    "        if ablation_method == \"resample_uniform\": \n",
    "            original_fact, corrupted_facts, _ = ablations.resample_ablation_uniform(hooked_model, prompt,subject,target,                                                             n_noise_samples=args.noise_samples)\n",
    "        elif ablation_method==\"resample\":\n",
    "            original_fact, corrupted_facts, _ = ablations.resample_ablation(hooked_model, prompt, subject, target, n_noise_samples=args.noise_samples, temperature=args.temperature)\n",
    "        elif ablation_method==\"noise\": \n",
    "            original_fact, corrupted_facts, _ = ablations.noise_ablation(hooked_model, prompt,subject,target,n_noise_samples=args.noise_samples)\n",
    "        else: \n",
    "            raise ValueError(\"ablation_method not recognized\")\n",
    "        return original_fact, corrupted_facts\n",
    "    \n",
    "    try:\n",
    "        original_fact, corrupted_facts = timeout_resample(args.ablation_method)\n",
    "    except TimeoutError:\n",
    "        warnings.warn(f\"Resample timed out for prompt {prompt}\")\n",
    "        \n",
    "        return None\n",
    "        \n",
    "        \n",
    "    #----------------------------------Grafting--------------------------------------#\n",
    "    graft_args = {        \n",
    "        \"model\":hooked_model,\n",
    "        \"device\":args.device,\n",
    "        \"clean_prompt\":original_fact,\n",
    "        \"corrupted_prompts\":corrupted_facts,\n",
    "        \"target\":target,\n",
    "        \"use_mle_token_graft\":args.use_mle_token_graft,\n",
    "        \"graft_threshold\":args.graft_threshold,\n",
    "    }\n",
    "    \n",
    "    if args.graft_type == \"decomposition\":\n",
    "        graft = activation_grafts.DecompositionGraft(**graft_args)\n",
    "    elif args.graft_type == \"causal_total_effect\":\n",
    "        graft = activation_grafts.CausalTotalEffectGraft(**graft_args)\n",
    "    elif args.graft_type == \"causal_total_effect_window\":\n",
    "        graft_args[\"window_size\"] = args.window_size\n",
    "        graft_args[\"window_stride\"] = args.window_stride\n",
    "        graft = activation_grafts.CausalTotalEffectWindowGraft(**graft_args)\n",
    "    elif args.graft_type == \"causal_direct_effect_window\":\n",
    "        raise NotImplementedError(\"Causal Direct Effect Window Graft not implemented\")\n",
    "\n",
    "    graft.run()\n",
    "    lora_configs = graft.generate_lora_configs(args)\n",
    "    if len(lora_configs) == 0:\n",
    "        warnings.warn(\"No LoRA configs generated\")\n",
    "    print(lora_configs)\n",
    "    \n",
    "    \n",
    "    #--------------------------------Setup logging-----------------------------------#\n",
    "    if args.do_wandb: \n",
    "        run = wandb.init(\n",
    "            project=f\"test_lora\",\n",
    "            name=f\"{prompt.format(subject)} + {target} -> {target_new}\",\n",
    "            config=vars(args),\n",
    "        )\n",
    "        \n",
    "    #---------------------------------Setup Model------------------------------------#\n",
    "    if args.model_name == \"gpt2-small\":\n",
    "        hf_model_name = \"gpt2\"\n",
    "        n_layer = 12\n",
    "        config = GPT2Config(\n",
    "            n_embd=768, n_layer=n_layer, n_head=12, \n",
    "        )\n",
    "    elif args.model_name == \"gpt2-large\":\n",
    "        hf_model_name = args.model_name\n",
    "        n_layer = 35\n",
    "        config = GPT2Config(\n",
    "            n_embd=1280, n_layer=n_layer, n_head=20, \n",
    "        )\n",
    "    else: \n",
    "        raise ValueError(\"model_name not recognized\")\n",
    "    print(config)\n",
    "    \n",
    "    del hooked_model\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.set_grad_enabled(True)\n",
    "    print(\"..initializing model\")\n",
    "    lm_net = GPT2LMModel(config, lora_configs)\n",
    "    print(\"a\")\n",
    "    model = GPT2LMHeadModel.from_pretrained(hf_model_name)\n",
    "    print(\"b\")\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(hf_model_name)\n",
    "    print(\"c\")\n",
    "    state_dict = model.state_dict()\n",
    "    print(\"d\")\n",
    "    lm_net.load_weight(state_dict)  \n",
    "    \n",
    "    #-----------------------------Setup Traininable Parameters---------------------------------#\n",
    "    print(\"setting trainable parameters\")\n",
    "    if \"lora\" in args.task: \n",
    "        lora.mark_only_lora_as_trainable(lm_net)\n",
    "    elif args.task==\"finetune\":\n",
    "        set_all_trainable(lm_net)\n",
    "    elif args.task==\"graft_finetune\":\n",
    "        set_trainable_from_graft(lm_net, graft)\n",
    "    else:\n",
    "        raise ValueError(\"Task not recognized\")\n",
    "            \n",
    "    print(\"creating datasets\")\n",
    "    if args.fp16:\n",
    "        try:\n",
    "            from torch.cuda import amp\n",
    "        except Exception as e:\n",
    "            warnings.warn('Could not import amp, apex may not be installed')\n",
    "    \n",
    "    \n",
    "    if args.rank == 0:\n",
    "        work_dir = os.getenv('PT_OUTPUT_DIR', 'gpt2_model')\n",
    "        args.logging = create_exp_dir(work_dir)\n",
    "    \n",
    "    \n",
    "    dataset = create_lm_dataset(\n",
    "        prompts=training_prompts, target=target_new,\n",
    "        subject=subject, tokenizer=tokenizer, args=args\n",
    "    )\n",
    "    dataset_ref = create_lm_dataset(\n",
    "        prompts=reference_evaluation_prompts, target=target,\n",
    "        subject=subject, tokenizer=tokenizer, args=args\n",
    "    )\n",
    "    \n",
    "    neighbourhood_dataset = create_lm_dataset(\n",
    "        prompts=neighborhood_prompts, target=target,\n",
    "        subject=subject, tokenizer=tokenizer, args=args\n",
    "    )\n",
    "    neighbourhood_dataset_ref = create_lm_dataset(\n",
    "        prompts=reference_neighborhood_prompts, target=target_new,\n",
    "        subject=subject, tokenizer=tokenizer, args=args\n",
    "    )\n",
    "    \n",
    "    same_attribute_dataset = create_lm_dataset(\n",
    "        prompts=same_attribute_prompts, target=target_new,\n",
    "        subject=subject, tokenizer=tokenizer, args=args\n",
    "    )\n",
    "    same_attribute_dataset = create_lm_dataset(\n",
    "        prompts=same_attribute_prompts, target=target, \n",
    "        subject=subject, tokenizer=tokenizer, args=args\n",
    "    )\n",
    "    same_attribute_dataset_ref = create_lm_dataset(\n",
    "        prompts=reference_same_attribute_prompts, target=target_new, \n",
    "        subject=subject, tokenizer=tokenizer, args=args\n",
    "    )\n",
    "    dataset_indices = list(range(len(dataset)))\n",
    "    training_indices, valid_indices = train_test_split(\n",
    "        dataset_indices, test_size=args.test_size, random_state=args.random_seed\n",
    "    )\n",
    "    training_prompts = [d for i,d in enumerate(dataset) if i in training_indices]\n",
    "    valid_prompts = [d for i,d in enumerate(dataset) if i in valid_indices]\n",
    "    training_prompts_ref = [d for i,d in enumerate(dataset_ref) if i in training_indices]\n",
    "    valid_prompts_ref = [d for i,d in enumerate(dataset_ref) if i in valid_indices]\n",
    "    \n",
    "    \n",
    "    train_data = FT_Dataset(\n",
    "        samples=training_prompts,\n",
    "        ref_samples=training_prompts_ref,\n",
    "        batch_size=args.train_batch_size,\n",
    "        max_seq_length=args.seq_len, \n",
    "        joint_lm=args.obj=='jlm'\n",
    "    ) \n",
    "    valid_data = FT_Dataset(\n",
    "        samples=valid_prompts,\n",
    "        ref_samples=valid_prompts_ref,\n",
    "        batch_size=args.train_batch_size,\n",
    "        max_seq_length=args.seq_len, \n",
    "        joint_lm=args.obj=='jlm'\n",
    "    )     \n",
    "    neighbourhood_data = FT_Dataset(\n",
    "        samples=neighbourhood_dataset,\n",
    "        ref_samples=neighbourhood_dataset_ref,\n",
    "        batch_size=args.train_batch_size,\n",
    "        max_seq_length=args.seq_len, \n",
    "        joint_lm=args.obj=='jlm',\n",
    "    )\n",
    "    same_attribute_data = FT_Dataset(\n",
    "        samples=same_attribute_dataset,\n",
    "        ref_samples=same_attribute_dataset_ref,\n",
    "        batch_size=args.train_batch_size,\n",
    "        max_seq_length=args.seq_len, \n",
    "        joint_lm=args.obj=='jlm'\n",
    "    )        \n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_data, batch_size=args.train_batch_size, num_workers=0, \n",
    "        shuffle=False, pin_memory=False, drop_last=True,\n",
    "    )\n",
    "    valid_loader = DataLoader(\n",
    "        valid_data, batch_size=args.valid_batch_size, num_workers=0, \n",
    "        shuffle=False, pin_memory=False, drop_last=False,\n",
    "    )\n",
    "    neighbourhood_loader = DataLoader(\n",
    "        neighbourhood_data, batch_size=len(neighbourhood_data), num_workers=0, \n",
    "        shuffle=False, pin_memory=False, drop_last=False,\n",
    "    )\n",
    "    same_attribute_loader = DataLoader(\n",
    "        same_attribute_data, batch_size=len(same_attribute_data), num_workers=0, \n",
    "        shuffle=False, pin_memory=False, drop_last=False,\n",
    "    )\n",
    "    \n",
    "    if args.init_checkpoint is not None:\n",
    "        print('loading model pretrained weight.')\n",
    "        lm_net.load_weight(torch.load(args.init_checkpoint))    \n",
    "        \n",
    "    if args.device=='cuda':\n",
    "        print('using cuda.')\n",
    "        lm_net = lm_net.cuda()\n",
    "\n",
    "    optimizer = create_adam_optimizer_from_args(lm_net, args)\n",
    "    \n",
    "    print(\"eval\")\n",
    "    test_evaluation = evaluate(lm_net,valid_loader,args,tokenizer,)\n",
    "    test_evaluation = {f\"testing_{k}\": v for k, v in test_evaluation.items()}\n",
    "    #evaluating specificity\n",
    "    neighbourhood_evaluation = evaluate(lm_net,neighbourhood_loader,args,tokenizer)\n",
    "    neighbourhood_evaluation = {f\"neighbourhood_{k}\": v for k, v in neighbourhood_evaluation.items()}\n",
    "\n",
    "    same_attribute_evaluation = evaluate(lm_net,same_attribute_loader,args,tokenizer)\n",
    "    same_attribute_evaluation = {f\"same_attribute_{k}\": v for k, v in same_attribute_evaluation.items()}\n",
    "    \n",
    "    init_evaluation = {**test_evaluation, **neighbourhood_evaluation, **same_attribute_evaluation}\n",
    "    \n",
    "    if args.max_step is None:\n",
    "        args.max_step = (args.max_epoch * train_data.num_batches) \n",
    "        print('set max_step:', args.max_step)\n",
    "    print(\"Training\")\n",
    "    scheduler = create_optimizer_scheduler(optimizer, args)\n",
    "    if args.fp16:\n",
    "        lm_net, optimizer = amp.initialize(lm_net, optimizer, opt_level=\"O1\")\n",
    "    try:\n",
    "        train_step = 0\n",
    "        for epoch in itertools.count(start=1):\n",
    "            train_step = train_validate(\n",
    "                lm_net, optimizer, scheduler, train_loader, valid_loader, args, \n",
    "                train_step=train_step, epoch=epoch\n",
    "            )\n",
    "            if train_step >= args.max_step or (args.max_epoch is not None and epoch >= args.max_epoch):\n",
    "                if args.rank == 0:\n",
    "                    print('-' * 100)\n",
    "                    print('End of training')\n",
    "                break\n",
    "    except KeyboardInterrupt:\n",
    "        if args.rank == 0:\n",
    "            print('-' * 100)\n",
    "            print('Exiting from training early')\n",
    "        early_exit = True\n",
    "\n",
    "    print(\"eval\")\n",
    "    test_evaluation = evaluate(lm_net,valid_loader,args,tokenizer,)\n",
    "    test_evaluation = {f\"testing_{k}\": v for k, v in test_evaluation.items()}\n",
    "    #evaluating specificity\n",
    "    neighbourhood_evaluation = evaluate(lm_net,neighbourhood_loader,args,tokenizer)\n",
    "    neighbourhood_evaluation = {f\"neighbourhood_{k}\": v for k, v in neighbourhood_evaluation.items()}\n",
    "\n",
    "    same_attribute_evaluation = evaluate(lm_net,same_attribute_loader,args,tokenizer)\n",
    "    same_attribute_evaluation = {f\"same_attribute_{k}\": v for k, v in same_attribute_evaluation.items()}\n",
    "    \n",
    "    total_evaluation = {**test_evaluation, **neighbourhood_evaluation, **same_attribute_evaluation}\n",
    "        \n",
    "    if args.do_wandb: \n",
    "        run.finish()\n",
    "       \n",
    "\n",
    "    del lm_net\n",
    "    del state_dict\n",
    "    del model\n",
    "    del optimizer\n",
    "    del scheduler\n",
    "    del tokenizer\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return {\n",
    "        \"early_exit\" : early_exit,\n",
    "        \"original_fact\" : original_fact,\n",
    "        \"target\" : target,\n",
    "        \"target_new\" : target_new,\n",
    "        \"total_evaluation\" : total_evaluation,\n",
    "        \"init_evaluation\" : init_evaluation,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(args): \n",
    "    correction_dataset = CorrectionDataset(args.fact_data)\n",
    "    correction_dataloader = DataLoader(correction_dataset, batch_size=1)\n",
    "    early_exit = False\n",
    "    \n",
    "    all_evaluations = [] ; all_init_evaluations = []\n",
    "    all_prompts = [] ; all_target = [] ; all_target_new = []\n",
    "    \n",
    "    for batch_idx, batch in enumerate(correction_dataloader):\n",
    "        \n",
    "        results = run_prompt(batch, args)\n",
    "        if results is None:\n",
    "            continue\n",
    "        \n",
    "        early_exit = results[\"early_exit\"]\n",
    "        if early_exit:\n",
    "            break\n",
    "        \n",
    "        all_prompts.append(results[\"original_fact\"])\n",
    "        all_target.append(results[\"target\"])\n",
    "        all_target_new.append(results[\"target_new\"])\n",
    "        all_evaluations.append(results[\"total_evaluation\"])\n",
    "        all_init_evaluations.append(results[\"init_evaluation\"])\n",
    "        \n",
    "    log_experiment(all_prompts, all_target, all_target_new, all_evaluations, all_init_evaluations, args)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    args = parse_args()\n",
    "    key=\"7b93b93f91b9088eb5e2a52295c51c5d6d9fd2e3\"\n",
    "    if args.do_wandb: \n",
    "        wandb.login(key=key)\n",
    "        \n",
    "    random.seed(args.random_seed)\n",
    "    torch.manual_seed(args.random_seed)   \n",
    "\n",
    "    run_experiment(args)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
