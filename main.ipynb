{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: einops>=0.6.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (0.6.1)\n",
      "Requirement already satisfied: PyYAML in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (6.0)\n",
      "Requirement already satisfied: torch>=1.10 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (2.0.1)\n",
      "Requirement already satisfied: datasets>=2.14.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (2.14.4)\n",
      "Requirement already satisfied: numpy>=1.25.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (1.25.2)\n",
      "Requirement already satisfied: transformers>=4.31.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (4.33.0)\n",
      "Requirement already satisfied: tqdm>=4.66.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (4.66.1)\n",
      "Requirement already satisfied: pandas>=2.0.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (2.1.0)\n",
      "Requirement already satisfied: wandb>=0.15.8 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from -r requirements.txt (line 9)) (0.15.9)\n",
      "Requirement already satisfied: fancy-einsum>=0.0.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from -r requirements.txt (line 10)) (0.0.3)\n",
      "Requirement already satisfied: jaxtyping in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from -r requirements.txt (line 11)) (0.2.21)\n",
      "Requirement already satisfied: rich>=13.5.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from -r requirements.txt (line 12)) (13.5.2)\n",
      "Requirement already satisfied: scikit-learn in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from -r requirements.txt (line 13)) (1.3.0)\n",
      "Requirement already satisfied: accelerate in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from -r requirements.txt (line 14)) (0.22.0)\n",
      "Requirement already satisfied: timeout-decorator in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from -r requirements.txt (line 15)) (0.5.0)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.10->-r requirements.txt (line 3)) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.10->-r requirements.txt (line 3)) (4.7.1)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.10->-r requirements.txt (line 3)) (1.12)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.10->-r requirements.txt (line 3)) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.10->-r requirements.txt (line 3)) (3.1.2)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets>=2.14.4->-r requirements.txt (line 4)) (12.0.1)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets>=2.14.4->-r requirements.txt (line 4)) (0.3.6)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets>=2.14.4->-r requirements.txt (line 4)) (2.31.0)\n",
      "Requirement already satisfied: xxhash in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets>=2.14.4->-r requirements.txt (line 4)) (3.3.0)\n",
      "Requirement already satisfied: multiprocess in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets>=2.14.4->-r requirements.txt (line 4)) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets>=2.14.4->-r requirements.txt (line 4)) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets>=2.14.4->-r requirements.txt (line 4)) (3.8.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets>=2.14.4->-r requirements.txt (line 4)) (0.16.4)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets>=2.14.4->-r requirements.txt (line 4)) (21.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers>=4.31.0->-r requirements.txt (line 6)) (2023.8.8)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers>=4.31.0->-r requirements.txt (line 6)) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers>=4.31.0->-r requirements.txt (line 6)) (0.3.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas>=2.0.3->-r requirements.txt (line 8)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas>=2.0.3->-r requirements.txt (line 8)) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas>=2.0.3->-r requirements.txt (line 8)) (2023.3)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb>=0.15.8->-r requirements.txt (line 9)) (8.1.6)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb>=0.15.8->-r requirements.txt (line 9)) (3.1.34)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb>=0.15.8->-r requirements.txt (line 9)) (5.9.5)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb>=0.15.8->-r requirements.txt (line 9)) (1.30.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb>=0.15.8->-r requirements.txt (line 9)) (0.4.0)\n",
      "Requirement already satisfied: pathtools in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb>=0.15.8->-r requirements.txt (line 9)) (0.1.2)\n",
      "Requirement already satisfied: setproctitle in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb>=0.15.8->-r requirements.txt (line 9)) (1.3.2)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb>=0.15.8->-r requirements.txt (line 9)) (68.0.0)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb>=0.15.8->-r requirements.txt (line 9)) (1.4.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb>=0.15.8->-r requirements.txt (line 9)) (4.23.4)\n",
      "Requirement already satisfied: typeguard>=2.13.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jaxtyping->-r requirements.txt (line 11)) (4.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from rich>=13.5.2->-r requirements.txt (line 12)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from rich>=13.5.2->-r requirements.txt (line 12)) (2.15.1)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-learn->-r requirements.txt (line 13)) (1.11.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-learn->-r requirements.txt (line 13)) (1.3.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-learn->-r requirements.txt (line 13)) (3.2.0)\n",
      "Requirement already satisfied: six>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb>=0.15.8->-r requirements.txt (line 9)) (1.16.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets>=2.14.4->-r requirements.txt (line 4)) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets>=2.14.4->-r requirements.txt (line 4)) (3.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets>=2.14.4->-r requirements.txt (line 4)) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets>=2.14.4->-r requirements.txt (line 4)) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets>=2.14.4->-r requirements.txt (line 4)) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets>=2.14.4->-r requirements.txt (line 4)) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets>=2.14.4->-r requirements.txt (line 4)) (1.3.1)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb>=0.15.8->-r requirements.txt (line 9)) (4.0.10)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=13.5.2->-r requirements.txt (line 12)) (0.1.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging->datasets>=2.14.4->-r requirements.txt (line 4)) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.19.0->datasets>=2.14.4->-r requirements.txt (line 4)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.19.0->datasets>=2.14.4->-r requirements.txt (line 4)) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.19.0->datasets>=2.14.4->-r requirements.txt (line 4)) (2023.5.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jinja2->torch>=1.10->-r requirements.txt (line 3)) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sympy->torch>=1.10->-r requirements.txt (line 3)) (1.3.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb>=0.15.8->-r requirements.txt (line 9)) (5.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import types\n",
    "import wandb\n",
    "import torch\n",
    "import random\n",
    "import argparse\n",
    "import warnings\n",
    "import itertools\n",
    "import warnings\n",
    "import loralib as lora\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from transformer_lens import HookedTransformer\n",
    "from gpt2_lora.data_utils import FT_Dataset\n",
    "from gpt2_lora.model import GPT2LMModel, GPT2Config\n",
    "from gpt2_lora.training.train import train_validate\n",
    "from gpt2_lora.correction_dataset import CorrectionDataset, create_lm_dataset\n",
    "import gpt2_lora.ablations as ablations\n",
    "import gpt2_lora.activation_graft as activation_grafts\n",
    "from gpt2_lora.training.optimizer import (\n",
    "    create_optimizer_scheduler, \n",
    "    add_optimizer_params, \n",
    "    create_adam_optimizer_from_args\n",
    ")\n",
    "from gpt2_lora.exp_utils import create_exp_dir\n",
    "from gpt2_lora.training.evaluate import evaluate\n",
    "from gpt2_lora.utils import set_all_trainable, set_trainable_from_graft, AverageMeter, log_experiment\n",
    "from sklearn.model_selection import train_test_split\n",
    "from timeout_decorator import timeout, TimeoutError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_args(args):\n",
    "    if args.rank == 0:\n",
    "        print('=' * 100)\n",
    "        for k, v in args.__dict__.items():\n",
    "            print(f'        - {k} : {v}')\n",
    "        print('=' * 100)\n",
    "        \n",
    "def get_device():\n",
    "    return \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def validate_args(args):\n",
    "    if args.task not in ['lora_graft_finetune', 'lora_mlp_finetune', 'lora_attn_finetune', 'lora_all_finetune', 'finetune', 'graft_finetune']: \n",
    "        raise ValueError(\"task not recognized\")\n",
    "    if args.task==\"lora_graft_finetune\": \n",
    "        if sum([args.adapt_mlp_c_fc, args.adapt_mlp_c_proj, args.adapt_attn_c_attn, args.adapt_attn_c_proj]) == 0: \n",
    "            raise ValueError(\"No LoRA layers selected\")\n",
    "    if args.task==\"lora_mlp_finetune\": \n",
    "        if sum([args.adapt_mlp_c_fc, args.adapt_mlp_c_proj]) == 0: \n",
    "            raise ValueError(\"No LoRA MLP layers selected\")\n",
    "    if args.task==\"lora_attn_finetune\": \n",
    "        if sum([args.aadapt_attn_c_attn, args.adapt_attn_c_proj]) == 0: \n",
    "            raise ValueError(\"No LoRA Attention layers selected\")\n",
    "    if args.graft_type not in [\"decomposition\", \"causal_total_effect\", \"causal_total_effect_window\", \"causal_direct_effect_window\"]: \n",
    "        raise ValueError(\"graft_type not recognized\")\n",
    "    if args.ablation_method not in [\"noise\", \"resample\", \"resample_uniform\"]: \n",
    "        raise ValueError(\"ablation_method not recognized\")\n",
    "    \n",
    "\n",
    "def parse_args(config_path=\"configs/config_lora_compensatory.yaml\"):\n",
    "    with open(config_path, 'r') as config_file:\n",
    "        config = yaml.safe_load(config_file)\n",
    "\n",
    "    args = types.SimpleNamespace()\n",
    "    \n",
    "    for key, value in config.items():\n",
    "        setattr(args, key, value)\n",
    "\n",
    "    setattr(args, 'device', get_device())      \n",
    "    validate_args(args)\n",
    "    print_args(args)\n",
    "    \n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_prompt(batch, args): \n",
    "    early_exit = False\n",
    "    torch.set_grad_enabled(False)\n",
    "    hooked_model = HookedTransformer.from_pretrained(\n",
    "        args.model_name,\n",
    "        center_unembed=True,  \n",
    "        center_writing_weights=True,              # Whether to center weights writing to the residual stream (ie set mean to be zero). Due to LayerNorm this doesn't change the computation.      \n",
    "        fold_ln=True,                             # Whether to  fold in the LayerNorm weights to the subsequent linear layer.\n",
    "        refactor_factored_attn_matrices=True,\n",
    "    )\n",
    "    #----------------------------Prepare Correction Dataset-----------------------------#\n",
    "    prompt = batch[\"prompt\"][0]\n",
    "    subject = batch[\"subject\"][0]\n",
    "    target = batch[\"target\"][0]\n",
    "    target_new = batch[\"target_new\"][0]\n",
    "    training_prompts = [p[0] for p in batch[\"training_prompts\"]]\n",
    "    reference_evaluation_prompts = [p[0] for p in batch[\"reference_evaluation_prompts\"]]\n",
    "    neighborhood_prompts = [p[0] for p in batch[\"neighborhood_prompts\"]]\n",
    "    reference_neighborhood_prompts = [p[0] for p in batch[\"reference_neighborhood_prompts\"]]\n",
    "    same_attribute_prompts = [p[0] for p in batch[\"same_attribute_prompts\"]]\n",
    "    reference_same_attribute_prompts = [p[0] for p in batch[\"reference_same_attribute_prompts\"]]\n",
    "    \n",
    "    print(prompt)\n",
    "    print(subject)\n",
    "    print(target, target_new)\n",
    "    \n",
    "    @timeout(30)\n",
    "    def timeout_resample(ablation_method):\n",
    "        if ablation_method == \"resample_uniform\": \n",
    "            original_fact, corrupted_facts, _ = ablations.resample_ablation_uniform(hooked_model, prompt,subject,target,                                                             n_noise_samples=args.noise_samples)\n",
    "        elif ablation_method==\"resample\":\n",
    "            original_fact, corrupted_facts, _ = ablations.resample_ablation(hooked_model, prompt, subject, target, n_noise_samples=args.noise_samples, temperature=args.temperature)\n",
    "        elif ablation_method==\"noise\": \n",
    "            original_fact, corrupted_facts, _ = ablations.noise_ablation(hooked_model, prompt,subject,target,n_noise_samples=args.noise_samples)\n",
    "        else: \n",
    "            raise ValueError(\"ablation_method not recognized\")\n",
    "        return original_fact, corrupted_facts\n",
    "    \n",
    "    try:\n",
    "        original_fact, corrupted_facts = timeout_resample(args.ablation_method)\n",
    "    except TimeoutError:\n",
    "        warnings.warn(f\"Resample timed out for prompt {prompt}\")\n",
    "        \n",
    "        return None\n",
    "        \n",
    "        \n",
    "    #----------------------------------Grafting--------------------------------------#\n",
    "    graft_args = {        \n",
    "        \"model\":hooked_model,\n",
    "        \"device\":args.device,\n",
    "        \"clean_prompt\":original_fact,\n",
    "        \"corrupted_prompts\":corrupted_facts,\n",
    "        \"target\":target,\n",
    "        \"use_mle_token_graft\":args.use_mle_token_graft,\n",
    "        \"graft_threshold\":args.graft_threshold,\n",
    "    }\n",
    "    \n",
    "    if args.graft_type == \"decomposition\":\n",
    "        graft = activation_grafts.DecompositionGraft(**graft_args)\n",
    "    elif args.graft_type == \"causal_total_effect\":\n",
    "        graft = activation_grafts.CausalTotalEffectGraft(**graft_args)\n",
    "    elif args.graft_type == \"causal_total_effect_window\":\n",
    "        graft_args[\"window_size\"] = args.window_size\n",
    "        graft_args[\"window_stride\"] = args.window_stride\n",
    "        graft = activation_grafts.CausalTotalEffectWindowGraft(**graft_args)\n",
    "    elif args.graft_type == \"causal_direct_effect_window\":\n",
    "        raise NotImplementedError(\"Causal Direct Effect Window Graft not implemented\")\n",
    "\n",
    "    graft.run()\n",
    "    lora_configs = graft.generate_lora_configs(args)\n",
    "    if len(lora_configs) == 0:\n",
    "        warnings.warn(\"No LoRA configs generated\")\n",
    "    print(lora_configs)\n",
    "    \n",
    "    \n",
    "    #--------------------------------Setup logging-----------------------------------#\n",
    "    if args.do_wandb: \n",
    "        run = wandb.init(\n",
    "            project=f\"test_lora\",\n",
    "            name=f\"{prompt.format(subject)} + {target} -> {target_new}\",\n",
    "            config=vars(args),\n",
    "        )\n",
    "        \n",
    "    #---------------------------------Setup Model------------------------------------#\n",
    "    if args.model_name == \"gpt2-small\":\n",
    "        hf_model_name = \"gpt2\"\n",
    "        n_layer = 12\n",
    "        config = GPT2Config(\n",
    "            n_embd=768, n_layer=n_layer, n_head=12, \n",
    "        )\n",
    "    elif args.model_name == \"gpt2-large\":\n",
    "        hf_model_name = args.model_name\n",
    "        n_layer = 35\n",
    "        config = GPT2Config(\n",
    "            n_embd=1280, n_layer=n_layer, n_head=20, \n",
    "        )\n",
    "    else: \n",
    "        raise ValueError(\"model_name not recognized\")\n",
    "    print(config)\n",
    "    \n",
    "    del hooked_model\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.set_grad_enabled(True)\n",
    "    print(\"..initializing model\")\n",
    "    lm_net = GPT2LMModel(config, lora_configs)\n",
    "    print(\"a\")\n",
    "    model = GPT2LMHeadModel.from_pretrained(hf_model_name)\n",
    "    print(\"b\")\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(hf_model_name)\n",
    "    print(\"c\")\n",
    "    state_dict = model.state_dict()\n",
    "    print(\"d\")\n",
    "    lm_net.load_weight(state_dict)  \n",
    "    \n",
    "    #-----------------------------Setup Traininable Parameters---------------------------------#\n",
    "    print(\"setting trainable parameters\")\n",
    "    if \"lora\" in args.task: \n",
    "        lora.mark_only_lora_as_trainable(lm_net)\n",
    "    elif args.task==\"finetune\":\n",
    "        set_all_trainable(lm_net)\n",
    "    elif args.task==\"graft_finetune\":\n",
    "        set_trainable_from_graft(lm_net, graft)\n",
    "    else:\n",
    "        raise ValueError(\"Task not recognized\")\n",
    "            \n",
    "    print(\"creating datasets\")\n",
    "    if args.fp16:\n",
    "        try:\n",
    "            from torch.cuda import amp\n",
    "        except Exception as e:\n",
    "            warnings.warn('Could not import amp, apex may not be installed')\n",
    "    \n",
    "    \n",
    "    if args.rank == 0:\n",
    "        work_dir = os.getenv('PT_OUTPUT_DIR', 'gpt2_model')\n",
    "        args.logging = create_exp_dir(work_dir)\n",
    "    \n",
    "    \n",
    "    dataset = create_lm_dataset(\n",
    "        prompts=training_prompts, target=target_new,\n",
    "        subject=subject, tokenizer=tokenizer, args=args\n",
    "    )\n",
    "    dataset_ref = create_lm_dataset(\n",
    "        prompts=reference_evaluation_prompts, target=target,\n",
    "        subject=subject, tokenizer=tokenizer, args=args\n",
    "    )\n",
    "    \n",
    "    neighbourhood_dataset = create_lm_dataset(\n",
    "        prompts=neighborhood_prompts, target=target,\n",
    "        subject=subject, tokenizer=tokenizer, args=args\n",
    "    )\n",
    "    neighbourhood_dataset_ref = create_lm_dataset(\n",
    "        prompts=reference_neighborhood_prompts, target=target_new,\n",
    "        subject=subject, tokenizer=tokenizer, args=args\n",
    "    )\n",
    "    \n",
    "    same_attribute_dataset = create_lm_dataset(\n",
    "        prompts=same_attribute_prompts, target=target_new,\n",
    "        subject=subject, tokenizer=tokenizer, args=args\n",
    "    )\n",
    "    same_attribute_dataset = create_lm_dataset(\n",
    "        prompts=same_attribute_prompts, target=target, \n",
    "        subject=subject, tokenizer=tokenizer, args=args\n",
    "    )\n",
    "    same_attribute_dataset_ref = create_lm_dataset(\n",
    "        prompts=reference_same_attribute_prompts, target=target_new, \n",
    "        subject=subject, tokenizer=tokenizer, args=args\n",
    "    )\n",
    "    dataset_indices = list(range(len(dataset)))\n",
    "    training_indices, valid_indices = train_test_split(\n",
    "        dataset_indices, test_size=args.test_size, random_state=args.random_seed\n",
    "    )\n",
    "    training_prompts = [d for i,d in enumerate(dataset) if i in training_indices]\n",
    "    valid_prompts = [d for i,d in enumerate(dataset) if i in valid_indices]\n",
    "    training_prompts_ref = [d for i,d in enumerate(dataset_ref) if i in training_indices]\n",
    "    valid_prompts_ref = [d for i,d in enumerate(dataset_ref) if i in valid_indices]\n",
    "    \n",
    "    \n",
    "    train_data = FT_Dataset(\n",
    "        samples=training_prompts,\n",
    "        ref_samples=training_prompts_ref,\n",
    "        batch_size=args.train_batch_size,\n",
    "        max_seq_length=args.seq_len, \n",
    "        joint_lm=args.obj=='jlm'\n",
    "    ) \n",
    "    valid_data = FT_Dataset(\n",
    "        samples=valid_prompts,\n",
    "        ref_samples=valid_prompts_ref,\n",
    "        batch_size=args.train_batch_size,\n",
    "        max_seq_length=args.seq_len, \n",
    "        joint_lm=args.obj=='jlm'\n",
    "    )     \n",
    "    neighbourhood_data = FT_Dataset(\n",
    "        samples=neighbourhood_dataset,\n",
    "        ref_samples=neighbourhood_dataset_ref,\n",
    "        batch_size=args.train_batch_size,\n",
    "        max_seq_length=args.seq_len, \n",
    "        joint_lm=args.obj=='jlm',\n",
    "    )\n",
    "    same_attribute_data = FT_Dataset(\n",
    "        samples=same_attribute_dataset,\n",
    "        ref_samples=same_attribute_dataset_ref,\n",
    "        batch_size=args.train_batch_size,\n",
    "        max_seq_length=args.seq_len, \n",
    "        joint_lm=args.obj=='jlm'\n",
    "    )        \n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_data, batch_size=args.train_batch_size, num_workers=0, \n",
    "        shuffle=False, pin_memory=False, drop_last=True,\n",
    "    )\n",
    "    valid_loader = DataLoader(\n",
    "        valid_data, batch_size=args.valid_batch_size, num_workers=0, \n",
    "        shuffle=False, pin_memory=False, drop_last=False,\n",
    "    )\n",
    "    neighbourhood_loader = DataLoader(\n",
    "        neighbourhood_data, batch_size=len(neighbourhood_data), num_workers=0, \n",
    "        shuffle=False, pin_memory=False, drop_last=False,\n",
    "    )\n",
    "    same_attribute_loader = DataLoader(\n",
    "        same_attribute_data, batch_size=len(same_attribute_data), num_workers=0, \n",
    "        shuffle=False, pin_memory=False, drop_last=False,\n",
    "    )\n",
    "    \n",
    "    if args.init_checkpoint is not None:\n",
    "        print('loading model pretrained weight.')\n",
    "        lm_net.load_weight(torch.load(args.init_checkpoint))    \n",
    "        \n",
    "    if args.device=='cuda':\n",
    "        print('using cuda.')\n",
    "        lm_net = lm_net.cuda()\n",
    "\n",
    "    optimizer = create_adam_optimizer_from_args(lm_net, args)\n",
    "    \n",
    "    print(\"eval\")\n",
    "    test_evaluation = evaluate(lm_net,valid_loader,args,tokenizer,)\n",
    "    test_evaluation = {f\"testing_{k}\": v for k, v in test_evaluation.items()}\n",
    "    #evaluating specificity\n",
    "    neighbourhood_evaluation = evaluate(lm_net,neighbourhood_loader,args,tokenizer)\n",
    "    neighbourhood_evaluation = {f\"neighbourhood_{k}\": v for k, v in neighbourhood_evaluation.items()}\n",
    "\n",
    "    same_attribute_evaluation = evaluate(lm_net,same_attribute_loader,args,tokenizer)\n",
    "    same_attribute_evaluation = {f\"same_attribute_{k}\": v for k, v in same_attribute_evaluation.items()}\n",
    "    \n",
    "    init_evaluation = {**test_evaluation, **neighbourhood_evaluation, **same_attribute_evaluation}\n",
    "    \n",
    "    if args.max_step is None:\n",
    "        args.max_step = (args.max_epoch * train_data.num_batches) \n",
    "        print('set max_step:', args.max_step)\n",
    "    print(\"Training\")\n",
    "    scheduler = create_optimizer_scheduler(optimizer, args)\n",
    "    if args.fp16:\n",
    "        lm_net, optimizer = amp.initialize(lm_net, optimizer, opt_level=\"O1\")\n",
    "    try:\n",
    "        train_step = 0\n",
    "        for epoch in itertools.count(start=1):\n",
    "            train_step = train_validate(\n",
    "                lm_net, optimizer, scheduler, train_loader, valid_loader, args, \n",
    "                train_step=train_step, epoch=epoch\n",
    "            )\n",
    "            if train_step >= args.max_step or (args.max_epoch is not None and epoch >= args.max_epoch):\n",
    "                if args.rank == 0:\n",
    "                    print('-' * 100)\n",
    "                    print('End of training')\n",
    "                break\n",
    "    except KeyboardInterrupt:\n",
    "        if args.rank == 0:\n",
    "            print('-' * 100)\n",
    "            print('Exiting from training early')\n",
    "        early_exit = True\n",
    "\n",
    "    print(\"eval\")\n",
    "    test_evaluation = evaluate(lm_net,valid_loader,args,tokenizer,)\n",
    "    test_evaluation = {f\"testing_{k}\": v for k, v in test_evaluation.items()}\n",
    "    #evaluating specificity\n",
    "    neighbourhood_evaluation = evaluate(lm_net,neighbourhood_loader,args,tokenizer)\n",
    "    neighbourhood_evaluation = {f\"neighbourhood_{k}\": v for k, v in neighbourhood_evaluation.items()}\n",
    "\n",
    "    same_attribute_evaluation = evaluate(lm_net,same_attribute_loader,args,tokenizer)\n",
    "    same_attribute_evaluation = {f\"same_attribute_{k}\": v for k, v in same_attribute_evaluation.items()}\n",
    "    \n",
    "    total_evaluation = {**test_evaluation, **neighbourhood_evaluation, **same_attribute_evaluation}\n",
    "        \n",
    "    if args.do_wandb: \n",
    "        run.finish()\n",
    "       \n",
    "\n",
    "    del lm_net\n",
    "    del state_dict\n",
    "    del model\n",
    "    del optimizer\n",
    "    del scheduler\n",
    "    del tokenizer\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return {\n",
    "        \"early_exit\" : early_exit,\n",
    "        \"original_fact\" : original_fact,\n",
    "        \"target\" : target,\n",
    "        \"target_new\" : target_new,\n",
    "        \"total_evaluation\" : total_evaluation,\n",
    "        \"init_evaluation\" : init_evaluation,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_experiment(args, start_batch=0): \n",
    "    correction_dataset = CorrectionDataset(args.fact_data)\n",
    "    correction_dataloader = DataLoader(correction_dataset, batch_size=1)\n",
    "    early_exit = False\n",
    "    \n",
    "    all_evaluations = [] ; all_init_evaluations = []\n",
    "    all_prompts = [] ; all_target = [] ; all_target_new = []\n",
    "    \n",
    "    for batch_idx, batch in enumerate(correction_dataloader):\n",
    "        print(f\"RUNNING BATCH {batch_idx}\")\n",
    "        \n",
    "        results = run_prompt(batch, args)\n",
    "        if results is None:\n",
    "            continue\n",
    "        \n",
    "        early_exit = results[\"early_exit\"]\n",
    "        if early_exit:\n",
    "            break\n",
    "        \n",
    "        all_prompts.append(results[\"original_fact\"])\n",
    "        all_target.append(results[\"target\"])\n",
    "        all_target_new.append(results[\"target_new\"])\n",
    "        all_evaluations.append(results[\"total_evaluation\"])\n",
    "        all_init_evaluations.append(results[\"init_evaluation\"])\n",
    "        \n",
    "        log_experiment(all_prompts, all_target, all_target_new, all_evaluations, all_init_evaluations, args)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "        - experiment_name : lora_compensatory\n",
      "        - fact_data : datasets/chatgpt_fact_dataset_100.json\n",
      "        - task : lora_graft_finetune\n",
      "        - random_seed : 1\n",
      "        - model_name : gpt2-small\n",
      "        - use_hf_model : True\n",
      "        - ablation_method : resample_uniform\n",
      "        - use_mle_token_graft : False\n",
      "        - graft_type : decomposition\n",
      "        - noise_samples : 10\n",
      "        - graft_threshold : 0.75\n",
      "        - temperature : 0.85\n",
      "        - window_size : 5\n",
      "        - window_stride : 1\n",
      "        - do_wandb : False\n",
      "        - log_interval : 10\n",
      "        - eval_interval : 10\n",
      "        - save_interval : 500\n",
      "        - lora_dim : 2\n",
      "        - lora_alpha : 128\n",
      "        - lora_dropout : 0.2\n",
      "        - adapt_mlp_c_fc : True\n",
      "        - adapt_mlp_c_proj : True\n",
      "        - adapt_attn_c_attn : True\n",
      "        - adapt_attn_c_proj : True\n",
      "        - test_size : 0.1\n",
      "        - completion_size : 0.2\n",
      "        - train_batch_size : 16\n",
      "        - valid_batch_size : 4\n",
      "        - grad_acc : 1\n",
      "        - seq_len : 32\n",
      "        - max_epoch : 5\n",
      "        - lr : 0.0001\n",
      "        - weight_decay : 0.01\n",
      "        - correct_bias : True\n",
      "        - adam_epislon : 1e-6\n",
      "        - no_decay_bias : True\n",
      "        - adam_beta1 : 0.9\n",
      "        - adam_beta2 : 0.98\n",
      "        - scheduler : linear\n",
      "        - max_step : None\n",
      "        - warmup_step : 0\n",
      "        - i_steps : 0\n",
      "        - i_lrs : 0.00025\n",
      "        - init_checkpoint : None\n",
      "        - fp16 : False\n",
      "        - work_dir : None\n",
      "        - obj : clm\n",
      "        - label_smooth : 0.0\n",
      "        - roll_interval : -1\n",
      "        - roll_lr : 0.0001\n",
      "        - roll_step : 100\n",
      "        - eval_epoch : 1\n",
      "        - clip : 1.0\n",
      "        - rank : 0\n",
      "        - save_model : False\n",
      "        - device : cuda\n",
      "====================================================================================================\n",
      "RUNNING BATCH 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "The mother tongue of {} is\n",
      "Thomas Joannes Stieltjes\n",
      "Dutch English\n",
      "[{'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': <gpt2_lora.model.LORAConfig object at 0x7f32436b2290>, 'mlp': None}, {'attn': None, 'mlp': <gpt2_lora.model.LORAConfig object at 0x7f322d041d50>}, {'attn': None, 'mlp': None}]\n",
      "<gpt2_lora.model.GPT2Config object at 0x7f322d040160>\n",
      "..initializing model\n",
      "a\n",
      "b\n",
      "c\n",
      "d\n",
      "setting trainable parameters\n",
      "creating datasets\n",
      "Experiment dir : gpt2_model\n",
      "using cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/LoRA/gpt2_lora/correction_dataset.py:23: UserWarning: The subject or target does not seem to be in the prompt.\n",
      "  warnings.warn(\"The subject or target does not seem to be in the prompt.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average loss 10.906845229012626\n",
      "average loss 11.459768295288086\n",
      "average loss 11.056260108947754\n",
      "set max_step: 80\n",
      "Training\n",
      "start to train the model................ 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/LoRA/gpt2_lora/training/optimizer.py:141: UserWarning: This overload of addcdiv_ is deprecated:\n",
      "\taddcdiv_(Number value, Tensor tensor1, Tensor tensor2)\n",
      "Consider using one of the following signatures instead:\n",
      "\taddcdiv_(Tensor tensor1, Tensor tensor2, *, Number value) (Triggered internally at /opt/conda/conda-bld/pytorch_1686274778240/work/torch/csrc/utils/python_arg_parser.cpp:1485.)\n",
      "  p.data.addcdiv_(-step_size, exp_avg, denom)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 step       10 |     10 batches | lr 8.75e-05 | ms/batch 74.93 | loss 10.41 | avg loss 10.73 | ppl 45509.52\n",
      "average loss 10.04605497632708\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   1 at step       10 | time:  0.27s | valid loss 10.05 | valid ppl 23064.62 | best ppl 23064.62 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "start to train the model................ 2\n",
      "| epoch   2 step       20 |      5 batches | lr 7.5e-05 | ms/batch 33.12 | loss  8.45 | avg loss  8.88 | ppl 7162.91\n",
      "average loss 7.8774277823311945\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   2 at step       20 | time:  0.27s | valid loss  7.88 | valid ppl 2637.08 | best ppl 2637.08 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   2 step       30 |     15 batches | lr 6.25e-05 | ms/batch 93.58 | loss  7.26 | avg loss  7.63 | ppl 2067.58\n",
      "average loss 6.617888995579311\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   3 at step       30 | time:  0.27s | valid loss  6.62 | valid ppl 748.36 | best ppl 748.36 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "start to train the model................ 3\n",
      "| epoch   3 step       40 |     10 batches | lr 5e-05 | ms/batch 66.46 | loss  7.13 | avg loss  7.03 | ppl 1124.98\n",
      "average loss 6.387527533939907\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   4 at step       40 | time:  0.27s | valid loss  6.39 | valid ppl 594.39 | best ppl 594.39 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "start to train the model................ 4\n",
      "| epoch   4 step       50 |      5 batches | lr 3.75e-05 | ms/batch 33.75 | loss  6.82 | avg loss  6.82 | ppl 919.02\n",
      "average loss 6.283905233655657\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   5 at step       50 | time:  0.27s | valid loss  6.28 | valid ppl 535.88 | best ppl 535.88 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   4 step       60 |     15 batches | lr 2.5e-05 | ms/batch 93.81 | loss  6.76 | avg loss  6.76 | ppl 859.68\n",
      "average loss 6.174745968409947\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   6 at step       60 | time:  0.27s | valid loss  6.17 | valid ppl 480.46 | best ppl 480.46 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "start to train the model................ 5\n",
      "| epoch   5 step       70 |     10 batches | lr 1.25e-05 | ms/batch 67.12 | loss  6.85 | avg loss  6.68 | ppl 793.54\n",
      "average loss 6.093067305428641\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   7 at step       70 | time:  0.27s | valid loss  6.09 | valid ppl 442.78 | best ppl 442.78 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "End of training\n",
      "eval\n",
      "average loss 6.067638124738421\n",
      "average loss 11.062321662902832\n",
      "average loss 10.456042289733887\n",
      "RUNNING BATCH 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "{} was created by\n",
      "Apple A5\n",
      "Apple Google\n",
      "[{'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': <gpt2_lora.model.LORAConfig object at 0x7f3220574070>, 'mlp': None}, {'attn': <gpt2_lora.model.LORAConfig object at 0x7f32205740a0>, 'mlp': None}, {'attn': None, 'mlp': None}]\n",
      "<gpt2_lora.model.GPT2Config object at 0x7f3220574040>\n",
      "..initializing model\n",
      "a\n",
      "b\n",
      "c\n",
      "d\n",
      "setting trainable parameters\n",
      "creating datasets\n",
      "Experiment dir : gpt2_model\n",
      "using cuda.\n",
      "eval\n",
      "average loss 11.040110270182291\n",
      "average loss 11.045103073120117\n",
      "average loss 11.346426963806152\n",
      "Training\n",
      "start to train the model................ 1\n",
      "start to train the model................ 2\n",
      "| epoch   2 step       10 |      4 batches | lr 8.75e-05 | ms/batch 26.23 | loss 10.63 | avg loss 10.67 | ppl 43006.64\n",
      "average loss 10.389188448588053\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   1 at step       10 | time:  0.11s | valid loss 10.39 | valid ppl 32506.28 | best ppl 32506.28 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "start to train the model................ 3\n",
      "start to train the model................ 4\n",
      "| epoch   4 step       20 |      2 batches | lr 7.5e-05 | ms/batch 12.96 | loss  8.86 | avg loss  8.93 | ppl 7524.62\n",
      "average loss 8.426199277242025\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   2 at step       20 | time:  0.11s | valid loss  8.43 | valid ppl 4565.12 | best ppl 4565.12 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "start to train the model................ 5\n",
      "| epoch   5 step       30 |      6 batches | lr 6.25e-05 | ms/batch 39.00 | loss  8.08 | avg loss  7.80 | ppl 2440.63\n",
      "average loss 6.429296334584554\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   3 at step       30 | time:  0.11s | valid loss  6.43 | valid ppl 619.74 | best ppl 619.74 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "End of training\n",
      "eval\n",
      "average loss 6.429296334584554\n",
      "average loss 7.366196632385254\n",
      "average loss 10.467009544372559\n",
      "RUNNING BATCH 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "The headquarter of {} is located in\n",
      "Monell Chemical Senses Center\n",
      "Philadelphia Mumbai\n",
      "[{'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': <gpt2_lora.model.LORAConfig object at 0x7f32205749d0>, 'mlp': <gpt2_lora.model.LORAConfig object at 0x7f3220574c10>}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}]\n",
      "<gpt2_lora.model.GPT2Config object at 0x7f32205749a0>\n",
      "..initializing model\n",
      "a\n",
      "b\n",
      "c\n",
      "d\n",
      "setting trainable parameters\n",
      "creating datasets\n",
      "Experiment dir : gpt2_model\n",
      "using cuda.\n",
      "eval\n",
      "average loss 10.599125385284424\n",
      "average loss 10.812950134277344\n",
      "average loss 10.8475980758667\n",
      "Training\n",
      "start to train the model................ 1\n",
      "start to train the model................ 2\n",
      "start to train the model................ 3\n",
      "start to train the model................ 4\n",
      "| epoch   4 step       10 |      1 batches | lr 8.75e-05 | ms/batch  6.89 | loss  9.90 | avg loss  9.90 | ppl 19835.67\n",
      "average loss 9.582612037658691\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   1 at step       10 | time:  0.08s | valid loss  9.58 | valid ppl 14510.27 | best ppl 14510.27 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "start to train the model................ 5\n",
      "----------------------------------------------------------------------------------------------------\n",
      "End of training\n",
      "eval\n",
      "average loss 8.479159832000732\n",
      "average loss 10.714977264404297\n",
      "average loss 11.09039306640625\n",
      "RUNNING BATCH 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "The native language of {} is\n",
      "Symeon of Polotsk\n",
      "Russian French\n",
      "[{'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': <gpt2_lora.model.LORAConfig object at 0x7f32205776a0>, 'mlp': <gpt2_lora.model.LORAConfig object at 0x7f3220577670>}, {'attn': None, 'mlp': None}, {'attn': <gpt2_lora.model.LORAConfig object at 0x7f3220577460>, 'mlp': None}]\n",
      "<gpt2_lora.model.GPT2Config object at 0x7f3220577790>\n",
      "..initializing model\n",
      "a\n",
      "b\n",
      "c\n",
      "d\n",
      "setting trainable parameters\n",
      "creating datasets\n",
      "Experiment dir : gpt2_model\n",
      "using cuda.\n",
      "eval\n",
      "average loss 11.002131779988607\n",
      "average loss 11.419471740722656\n",
      "average loss 10.833788871765137\n",
      "Training\n",
      "start to train the model................ 1\n",
      "start to train the model................ 2\n",
      "| epoch   2 step       10 |      5 batches | lr 8.75e-05 | ms/batch 35.51 | loss 10.34 | avg loss 10.55 | ppl 38125.38\n",
      "average loss 10.1098051071167\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   1 at step       10 | time:  0.12s | valid loss 10.11 | valid ppl 24582.87 | best ppl 24582.87 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "start to train the model................ 3\n",
      "start to train the model................ 4\n",
      "| epoch   4 step       20 |      5 batches | lr 7.5e-05 | ms/batch 35.30 | loss  8.41 | avg loss  8.77 | ppl 6441.63\n",
      "average loss 8.021392822265625\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   2 at step       20 | time:  0.12s | valid loss  8.02 | valid ppl 3045.42 | best ppl 3045.42 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "start to train the model................ 5\n",
      "----------------------------------------------------------------------------------------------------\n",
      "End of training\n",
      "eval\n",
      "average loss 7.44354248046875\n",
      "average loss 11.080344200134277\n",
      "average loss 10.34366226196289\n",
      "RUNNING BATCH 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "The mother tongue of {} is\n",
      "Jean Galland\n",
      "French Russian\n",
      "[{'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': <gpt2_lora.model.LORAConfig object at 0x7f3220574b50>, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}]\n",
      "<gpt2_lora.model.GPT2Config object at 0x7f3220574ca0>\n",
      "..initializing model\n",
      "a\n",
      "b\n",
      "c\n",
      "d\n",
      "setting trainable parameters\n",
      "creating datasets\n",
      "Experiment dir : gpt2_model\n",
      "using cuda.\n",
      "eval\n",
      "average loss 10.95760202407837\n",
      "average loss 11.705574035644531\n",
      "average loss 10.952913284301758\n",
      "Training\n",
      "start to train the model................ 1\n",
      "start to train the model................ 2\n",
      "| epoch   2 step       10 |      3 batches | lr 8.75e-05 | ms/batch 18.19 | loss 10.71 | avg loss 10.78 | ppl 48028.78\n",
      "average loss 10.54992413520813\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   1 at step       10 | time:  0.14s | valid loss 10.55 | valid ppl 38174.54 | best ppl 38174.54 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "start to train the model................ 3\n",
      "| epoch   3 step       20 |      6 batches | lr 7.5e-05 | ms/batch 36.73 | loss  9.76 | avg loss 10.03 | ppl 22757.42\n",
      "average loss 9.30596661567688\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   2 at step       20 | time:  0.14s | valid loss  9.31 | valid ppl 11003.48 | best ppl 11003.48 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "start to train the model................ 4\n",
      "start to train the model................ 5\n",
      "| epoch   5 step       30 |      2 batches | lr 6.25e-05 | ms/batch 12.16 | loss  8.48 | avg loss  8.48 | ppl 4794.27\n",
      "average loss 7.658398389816284\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   3 at step       30 | time:  0.14s | valid loss  7.66 | valid ppl 2118.36 | best ppl 2118.36 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "End of training\n",
      "eval\n",
      "average loss 7.01723837852478\n",
      "average loss 12.222152709960938\n",
      "average loss 10.501554489135742\n",
      "RUNNING BATCH 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "{} is a product of\n",
      "Windows Embedded CE 6.0\n",
      "Microsoft IBM\n",
      "[{'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': <gpt2_lora.model.LORAConfig object at 0x7f3220577550>, 'mlp': None}, {'attn': None, 'mlp': None}]\n",
      "<gpt2_lora.model.GPT2Config object at 0x7f3220577280>\n",
      "..initializing model\n",
      "a\n",
      "b\n",
      "c\n",
      "d\n",
      "setting trainable parameters\n",
      "creating datasets\n",
      "Experiment dir : gpt2_model\n",
      "using cuda.\n",
      "eval\n",
      "average loss 11.113736152648926\n",
      "average loss 10.753663063049316\n",
      "average loss 10.702347755432129\n",
      "Training\n",
      "start to train the model................ 1\n",
      "start to train the model................ 2\n",
      "start to train the model................ 3\n",
      "start to train the model................ 4\n",
      "| epoch   4 step       10 |      1 batches | lr 8.75e-05 | ms/batch  6.07 | loss 10.67 | avg loss 10.67 | ppl 43244.17\n",
      "average loss 10.84833812713623\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   1 at step       10 | time:  0.07s | valid loss 10.85 | valid ppl 51448.58 | best ppl 51448.58 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "start to train the model................ 5\n",
      "----------------------------------------------------------------------------------------------------\n",
      "End of training\n",
      "eval\n",
      "average loss 10.520344734191895\n",
      "average loss 10.78286075592041\n",
      "average loss 10.680990219116211\n",
      "RUNNING BATCH 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "{}, a citizen of\n",
      "Nathuram Godse\n",
      "India Italy\n",
      "[{'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': <gpt2_lora.model.LORAConfig object at 0x7f3220575450>, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}]\n",
      "<gpt2_lora.model.GPT2Config object at 0x7f3220575000>\n",
      "..initializing model\n",
      "a\n",
      "b\n",
      "c\n",
      "d\n",
      "setting trainable parameters\n",
      "creating datasets\n",
      "Experiment dir : gpt2_model\n",
      "using cuda.\n",
      "eval\n",
      "average loss 11.35508918762207\n",
      "average loss 10.980607032775879\n",
      "average loss 10.744660377502441\n",
      "Training\n",
      "start to train the model................ 1\n",
      "start to train the model................ 2\n",
      "| epoch   2 step       10 |      5 batches | lr 8.75e-05 | ms/batch 30.54 | loss 11.00 | avg loss 11.04 | ppl 62458.12\n",
      "average loss 10.80601692199707\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   1 at step       10 | time:  0.11s | valid loss 10.81 | valid ppl 49316.64 | best ppl 49316.64 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "start to train the model................ 3\n",
      "start to train the model................ 4\n",
      "| epoch   4 step       20 |      5 batches | lr 7.5e-05 | ms/batch 30.61 | loss  9.29 | avg loss  9.56 | ppl 14245.64\n",
      "average loss 9.045646985371908\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   2 at step       20 | time:  0.11s | valid loss  9.05 | valid ppl 8481.54 | best ppl 8481.54 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "start to train the model................ 5\n",
      "----------------------------------------------------------------------------------------------------\n",
      "End of training\n",
      "eval\n",
      "average loss 7.895545482635498\n",
      "average loss 10.638810157775879\n",
      "average loss 10.827138900756836\n",
      "RUNNING BATCH 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "{} is a native speaker of\n",
      "Stefanos Stratigos\n",
      "Greek Dutch\n",
      "[{'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': <gpt2_lora.model.LORAConfig object at 0x7f3220576b90>, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}]\n",
      "<gpt2_lora.model.GPT2Config object at 0x7f3220577490>\n",
      "..initializing model\n",
      "a\n",
      "b\n",
      "c\n",
      "d\n",
      "setting trainable parameters\n",
      "creating datasets\n",
      "Experiment dir : gpt2_model\n",
      "using cuda.\n",
      "eval\n",
      "average loss 10.665302276611328\n",
      "average loss 11.371330261230469\n",
      "average loss 11.028648376464844\n",
      "Training\n",
      "start to train the model................ 1\n",
      "start to train the model................ 2\n",
      "| epoch   2 step       10 |      1 batches | lr 8.75e-05 | ms/batch  6.15 | loss 10.42 | avg loss 10.42 | ppl 33371.89\n",
      "average loss 10.404997634887696\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   1 at step       10 | time:  0.19s | valid loss 10.40 | valid ppl 33024.26 | best ppl 33024.26 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "start to train the model................ 3\n",
      "| epoch   3 step       20 |      2 batches | lr 7.5e-05 | ms/batch 12.67 | loss  9.92 | avg loss  9.94 | ppl 20847.64\n",
      "average loss 9.77922077178955\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   2 at step       20 | time:  0.19s | valid loss  9.78 | valid ppl 17662.88 | best ppl 17662.88 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "start to train the model................ 4\n",
      "| epoch   4 step       30 |      3 batches | lr 6.25e-05 | ms/batch 18.60 | loss  8.95 | avg loss  9.08 | ppl 8740.74\n",
      "average loss 8.712635135650634\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   3 at step       30 | time:  0.19s | valid loss  8.71 | valid ppl 6079.24 | best ppl 6079.24 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "start to train the model................ 5\n",
      "| epoch   5 step       40 |      4 batches | lr 5e-05 | ms/batch 24.66 | loss  7.96 | avg loss  8.13 | ppl 3409.41\n",
      "average loss 7.669488048553466\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   4 at step       40 | time:  0.18s | valid loss  7.67 | valid ppl 2141.98 | best ppl 2141.98 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "End of training\n",
      "eval\n",
      "average loss 7.288968658447265\n",
      "average loss 11.573782920837402\n",
      "average loss 10.580649375915527\n",
      "RUNNING BATCH 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "The official religion of {} is\n",
      "Jahangir\n",
      "Islam Judaism\n",
      "[{'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': <gpt2_lora.model.LORAConfig object at 0x7f32205752d0>, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}]\n",
      "<gpt2_lora.model.GPT2Config object at 0x7f32205754e0>\n",
      "..initializing model\n",
      "a\n",
      "b\n",
      "c\n",
      "d\n",
      "setting trainable parameters\n",
      "creating datasets\n",
      "Experiment dir : gpt2_model\n",
      "using cuda.\n",
      "eval\n",
      "average loss 10.63019347190857\n",
      "average loss 11.272177696228027\n",
      "average loss 10.98154067993164\n",
      "Training\n",
      "start to train the model................ 1\n",
      "start to train the model................ 2\n",
      "| epoch   2 step       10 |      3 batches | lr 8.75e-05 | ms/batch 19.17 | loss 10.81 | avg loss 10.62 | ppl 40894.44\n",
      "average loss 10.34918737411499\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   1 at step       10 | time:  0.15s | valid loss 10.35 | valid ppl 31231.65 | best ppl 31231.65 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "start to train the model................ 3\n",
      "| epoch   3 step       20 |      6 batches | lr 7.5e-05 | ms/batch 37.20 | loss  9.75 | avg loss  9.97 | ppl 21268.91\n",
      "average loss 9.376237869262695\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   2 at step       20 | time:  0.15s | valid loss  9.38 | valid ppl 11804.52 | best ppl 11804.52 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "start to train the model................ 4\n",
      "start to train the model................ 5\n",
      "| epoch   5 step       30 |      2 batches | lr 6.25e-05 | ms/batch 12.86 | loss  8.14 | avg loss  8.27 | ppl 3903.91\n",
      "average loss 7.910917162895203\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   3 at step       30 | time:  0.15s | valid loss  7.91 | valid ppl 2726.89 | best ppl 2726.89 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "End of training\n",
      "eval\n",
      "average loss 7.284825801849365\n",
      "average loss 11.097719192504883\n",
      "average loss 10.98487377166748\n",
      "RUNNING BATCH 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "{} is a native speaker of\n",
      "Gwen Stefani\n",
      "English French\n",
      "[{'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': <gpt2_lora.model.LORAConfig object at 0x7f322d06f550>}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}]\n",
      "<gpt2_lora.model.GPT2Config object at 0x7f322d06e3b0>\n",
      "..initializing model\n",
      "a\n",
      "b\n",
      "c\n",
      "d\n",
      "setting trainable parameters\n",
      "creating datasets\n",
      "Experiment dir : gpt2_model\n",
      "using cuda.\n",
      "eval\n",
      "average loss 10.970361709594727\n",
      "average loss 11.464670181274414\n",
      "average loss 11.303961753845215\n",
      "Training\n",
      "start to train the model................ 1\n",
      "start to train the model................ 2\n",
      "| epoch   2 step       10 |      4 batches | lr 8.75e-05 | ms/batch 27.34 | loss 10.53 | avg loss 10.66 | ppl 42735.76\n",
      "average loss 10.662978490193685\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   1 at step       10 | time:  0.12s | valid loss 10.66 | valid ppl 42743.76 | best ppl 42743.76 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "start to train the model................ 3\n",
      "start to train the model................ 4\n",
      "| epoch   4 step       20 |      2 batches | lr 7.5e-05 | ms/batch 13.51 | loss  9.82 | avg loss  9.77 | ppl 17554.09\n",
      "average loss 9.824300448099772\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   2 at step       20 | time:  0.12s | valid loss  9.82 | valid ppl 18477.34 | best ppl 18477.34 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "start to train the model................ 5\n",
      "| epoch   5 step       30 |      6 batches | lr 6.25e-05 | ms/batch 40.56 | loss  9.11 | avg loss  8.99 | ppl 8020.59\n",
      "average loss 8.692313035329184\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   3 at step       30 | time:  0.12s | valid loss  8.69 | valid ppl 5956.94 | best ppl 5956.94 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "End of training\n",
      "eval\n",
      "average loss 8.692313035329184\n",
      "average loss 11.5208158493042\n",
      "average loss 10.901297569274902\n",
      "RUNNING BATCH 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "The official religion of {} is\n",
      "Uwais Qarni\n",
      "Islam Buddhism\n",
      "[{'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': <gpt2_lora.model.LORAConfig object at 0x7f3220575720>, 'mlp': <gpt2_lora.model.LORAConfig object at 0x7f3220574580>}, {'attn': <gpt2_lora.model.LORAConfig object at 0x7f3220576ce0>, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}]\n",
      "<gpt2_lora.model.GPT2Config object at 0x7f3220575420>\n",
      "..initializing model\n",
      "a\n",
      "b\n",
      "c\n",
      "d\n",
      "setting trainable parameters\n",
      "creating datasets\n",
      "Experiment dir : gpt2_model\n",
      "using cuda.\n",
      "eval\n",
      "average loss 11.182441393534342\n",
      "average loss 11.41324234008789\n",
      "average loss 10.798218727111816\n",
      "Training\n",
      "start to train the model................ 1\n",
      "start to train the model................ 2\n",
      "| epoch   2 step       10 |      4 batches | lr 8.75e-05 | ms/batch 29.88 | loss 10.14 | avg loss 10.40 | ppl 32814.51\n",
      "average loss 9.970730463663736\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   1 at step       10 | time:  0.12s | valid loss  9.97 | valid ppl 21391.11 | best ppl 21391.11 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "start to train the model................ 3\n",
      "start to train the model................ 4\n",
      "| epoch   4 step       20 |      2 batches | lr 7.5e-05 | ms/batch 14.76 | loss  7.72 | avg loss  7.79 | ppl 2426.30\n",
      "average loss 7.456333001454671\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   2 at step       20 | time:  0.13s | valid loss  7.46 | valid ppl 1730.79 | best ppl 1730.79 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "start to train the model................ 5\n",
      "| epoch   5 step       30 |      6 batches | lr 6.25e-05 | ms/batch 44.60 | loss  6.89 | avg loss  6.95 | ppl 1044.42\n",
      "average loss 6.603469053904216\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   3 at step       30 | time:  0.12s | valid loss  6.60 | valid ppl 737.65 | best ppl 737.65 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "End of training\n",
      "eval\n",
      "average loss 6.603469053904216\n",
      "average loss 10.523472785949707\n",
      "average loss 10.874576568603516\n",
      "RUNNING BATCH 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "The headquarters of {} is in\n",
      "Northeastern University\n",
      "Boston Dublin\n",
      "[{'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': <gpt2_lora.model.LORAConfig object at 0x7f322d06dde0>, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}]\n",
      "<gpt2_lora.model.GPT2Config object at 0x7f322d06d510>\n",
      "..initializing model\n",
      "a\n",
      "b\n",
      "c\n",
      "d\n",
      "setting trainable parameters\n",
      "creating datasets\n",
      "Experiment dir : gpt2_model\n",
      "using cuda.\n",
      "eval\n",
      "average loss 11.065768559773764\n",
      "average loss 10.644901275634766\n",
      "average loss 10.732298851013184\n",
      "Training\n",
      "start to train the model................ 1\n",
      "start to train the model................ 2\n",
      "| epoch   2 step       10 |      4 batches | lr 8.75e-05 | ms/batch 25.18 | loss 10.76 | avg loss 10.83 | ppl 50655.64\n",
      "average loss 10.72917111714681\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   1 at step       10 | time:  0.12s | valid loss 10.73 | valid ppl 45668.82 | best ppl 45668.82 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "start to train the model................ 3\n",
      "start to train the model................ 4\n",
      "| epoch   4 step       20 |      2 batches | lr 7.5e-05 | ms/batch 12.38 | loss  9.66 | avg loss  9.70 | ppl 16344.91\n",
      "average loss 9.745525995890299\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   2 at step       20 | time:  0.12s | valid loss  9.75 | valid ppl 17077.65 | best ppl 17077.65 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "start to train the model................ 5\n",
      "| epoch   5 step       30 |      6 batches | lr 6.25e-05 | ms/batch 38.35 | loss  8.83 | avg loss  8.75 | ppl 6313.86\n",
      "average loss 8.510777791341146\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   3 at step       30 | time:  0.11s | valid loss  8.51 | valid ppl 4968.03 | best ppl 4968.03 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "End of training\n",
      "eval\n",
      "average loss 8.510777791341146\n",
      "average loss 11.056209564208984\n",
      "average loss 10.21784496307373\n",
      "RUNNING BATCH 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "The native language of {} is\n",
      "Cees Nooteboom\n",
      "Dutch Indonesian\n",
      "[{'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': <gpt2_lora.model.LORAConfig object at 0x7f3220574550>}, {'attn': None, 'mlp': None}]\n",
      "<gpt2_lora.model.GPT2Config object at 0x7f3220576bf0>\n",
      "..initializing model\n",
      "a\n",
      "b\n",
      "c\n",
      "d\n",
      "setting trainable parameters\n",
      "creating datasets\n",
      "Experiment dir : gpt2_model\n",
      "using cuda.\n",
      "eval\n",
      "average loss 11.002045631408691\n",
      "average loss 10.988970756530762\n",
      "average loss 11.255168914794922\n",
      "Training\n",
      "start to train the model................ 1\n",
      "start to train the model................ 2\n",
      "| epoch   2 step       10 |      3 batches | lr 8.75e-05 | ms/batch 18.09 | loss 10.88 | avg loss 10.91 | ppl 54596.60\n",
      "average loss 10.76955771446228\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   1 at step       10 | time:  0.15s | valid loss 10.77 | valid ppl 47550.98 | best ppl 47550.98 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "start to train the model................ 3\n",
      "| epoch   3 step       20 |      6 batches | lr 7.5e-05 | ms/batch 37.40 | loss 10.22 | avg loss 10.41 | ppl 33216.71\n",
      "average loss 10.050456047058105\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   2 at step       20 | time:  0.15s | valid loss 10.05 | valid ppl 23166.35 | best ppl 23166.35 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "start to train the model................ 4\n",
      "start to train the model................ 5\n",
      "| epoch   5 step       30 |      2 batches | lr 6.25e-05 | ms/batch 12.39 | loss  9.11 | avg loss  9.15 | ppl 9405.32\n",
      "average loss 8.944625616073608\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   3 at step       30 | time:  0.15s | valid loss  8.94 | valid ppl 7666.58 | best ppl 7666.58 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "End of training\n",
      "eval\n",
      "average loss 8.428016901016235\n",
      "average loss 11.148838996887207\n",
      "average loss 10.873642921447754\n",
      "RUNNING BATCH 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "The native language of {} is\n",
      "Pierre Messmer\n",
      "French English\n",
      "[{'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': <gpt2_lora.model.LORAConfig object at 0x7f322d06fee0>, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}]\n",
      "<gpt2_lora.model.GPT2Config object at 0x7f322d06c0d0>\n",
      "..initializing model\n",
      "a\n",
      "b\n",
      "c\n",
      "d\n",
      "setting trainable parameters\n",
      "creating datasets\n",
      "Experiment dir : gpt2_model\n",
      "using cuda.\n",
      "eval\n",
      "average loss 10.78021469116211\n",
      "average loss 10.836499214172363\n",
      "average loss 11.926387786865234\n",
      "Training\n",
      "start to train the model................ 1\n",
      "| epoch   1 step       10 |     10 batches | lr 8.75e-05 | ms/batch 63.78 | loss 10.67 | avg loss 10.74 | ppl 46121.78\n",
      "average loss 10.614401626586915\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   1 at step       10 | time:  0.19s | valid loss 10.61 | valid ppl 40717.03 | best ppl 40717.03 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "start to train the model................ 2\n",
      "| epoch   2 step       20 |     10 batches | lr 7.5e-05 | ms/batch 62.92 | loss 10.19 | avg loss 10.43 | ppl 33763.07\n",
      "average loss 10.040421867370606\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   2 at step       20 | time:  0.19s | valid loss 10.04 | valid ppl 22935.06 | best ppl 22935.06 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "start to train the model................ 3\n",
      "| epoch   3 step       30 |     10 batches | lr 6.25e-05 | ms/batch 63.42 | loss  9.15 | avg loss  9.62 | ppl 15136.29\n",
      "average loss 8.866684532165527\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   3 at step       30 | time:  0.19s | valid loss  8.87 | valid ppl 7091.73 | best ppl 7091.73 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "start to train the model................ 4\n",
      "| epoch   4 step       40 |     10 batches | lr 5e-05 | ms/batch 63.66 | loss  7.98 | avg loss  8.41 | ppl 4481.62\n",
      "average loss 7.67036542892456\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   4 at step       40 | time:  0.19s | valid loss  7.67 | valid ppl 2143.86 | best ppl 2143.86 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "start to train the model................ 5\n",
      "| epoch   5 step       50 |     10 batches | lr 3.75e-05 | ms/batch 63.80 | loss  7.27 | avg loss  7.46 | ppl 1741.87\n",
      "average loss 6.95400447845459\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   5 at step       50 | time:  0.19s | valid loss  6.95 | valid ppl 1047.34 | best ppl 1047.34 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "End of training\n",
      "eval\n",
      "average loss 6.95400447845459\n",
      "average loss 10.919641494750977\n",
      "average loss 11.39352798461914\n",
      "RUNNING BATCH 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "The mother tongue of {} is\n",
      "Ilya Ehrenburg\n",
      "Russian Dutch\n",
      "[{'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': <gpt2_lora.model.LORAConfig object at 0x7f322d06f970>, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': <gpt2_lora.model.LORAConfig object at 0x7f322d06f8b0>, 'mlp': None}]\n",
      "<gpt2_lora.model.GPT2Config object at 0x7f322d06d6f0>\n",
      "..initializing model\n",
      "a\n",
      "b\n",
      "c\n",
      "d\n",
      "setting trainable parameters\n",
      "creating datasets\n",
      "Experiment dir : gpt2_model\n",
      "using cuda.\n",
      "eval\n",
      "average loss 10.90229332447052\n",
      "average loss 11.078109741210938\n",
      "average loss 11.137231826782227\n",
      "Training\n",
      "start to train the model................ 1\n",
      "| epoch   1 step       10 |     10 batches | lr 8.75e-05 | ms/batch 67.40 | loss 10.53 | avg loss 10.79 | ppl 48400.62\n",
      "average loss 10.501134991645813\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   1 at step       10 | time:  0.31s | valid loss 10.50 | valid ppl 36356.74 | best ppl 36356.74 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "start to train the model................ 2\n",
      "| epoch   2 step       20 |      4 batches | lr 7.5e-05 | ms/batch 26.54 | loss  9.42 | avg loss  9.62 | ppl 15073.61\n",
      "average loss 9.273574233055115\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   2 at step       20 | time:  0.31s | valid loss  9.27 | valid ppl 10652.76 | best ppl 10652.76 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   2 step       30 |     14 batches | lr 6.25e-05 | ms/batch 97.35 | loss  7.89 | avg loss  8.56 | ppl 5207.62\n",
      "average loss 7.86624675989151\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   3 at step       30 | time:  0.31s | valid loss  7.87 | valid ppl 2607.76 | best ppl 2607.76 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "start to train the model................ 3\n",
      "| epoch   3 step       40 |      8 batches | lr 5e-05 | ms/batch 53.28 | loss  7.46 | avg loss  7.49 | ppl 1787.65\n",
      "average loss 7.4105284214019775\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   4 at step       40 | time:  0.31s | valid loss  7.41 | valid ppl 1653.30 | best ppl 1653.30 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "start to train the model................ 4\n",
      "| epoch   4 step       50 |      2 batches | lr 3.75e-05 | ms/batch 13.63 | loss  6.84 | avg loss  6.84 | ppl 934.14\n",
      "average loss 7.248352289199829\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   5 at step       50 | time:  0.31s | valid loss  7.25 | valid ppl 1405.79 | best ppl 1405.79 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   4 step       60 |     12 batches | lr 2.5e-05 | ms/batch 97.54 | loss  7.32 | avg loss  7.23 | ppl 1383.58\n",
      "average loss 7.17524379491806\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   6 at step       60 | time:  0.31s | valid loss  7.18 | valid ppl 1306.68 | best ppl 1306.68 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "start to train the model................ 5\n",
      "| epoch   5 step       70 |      6 batches | lr 1.25e-05 | ms/batch 40.36 | loss  7.20 | avg loss  7.01 | ppl 1109.75\n",
      "average loss 7.141517162322998\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   7 at step       70 | time:  0.31s | valid loss  7.14 | valid ppl 1263.34 | best ppl 1263.34 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   5 step       80 |     16 batches | lr 0 | ms/batch 97.43 | loss  6.98 | avg loss  7.13 | ppl 1248.28\n",
      "average loss 7.130704760551453\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   8 at step       80 | time:  0.31s | valid loss  7.13 | valid ppl 1249.76 | best ppl 1249.76 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "End of training\n",
      "eval\n",
      "average loss 7.130704760551453\n",
      "average loss 11.306198120117188\n",
      "average loss 11.592198371887207\n",
      "RUNNING BATCH 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "{} is a citizen of\n",
      "Haseeb Ahsan\n",
      "Pakistan Niger\n",
      "[{'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': <gpt2_lora.model.LORAConfig object at 0x7f323c968fd0>, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}]\n",
      "<gpt2_lora.model.GPT2Config object at 0x7f323c9692d0>\n",
      "..initializing model\n",
      "a\n",
      "b\n",
      "c\n",
      "d\n",
      "setting trainable parameters\n",
      "creating datasets\n",
      "Experiment dir : gpt2_model\n",
      "using cuda.\n",
      "eval\n",
      "average loss 11.01781940460205\n",
      "average loss 11.122780799865723\n",
      "average loss 10.982738494873047\n",
      "Training\n",
      "start to train the model................ 1\n",
      "start to train the model................ 2\n",
      "| epoch   2 step       10 |      4 batches | lr 8.75e-05 | ms/batch 25.47 | loss 10.92 | avg loss 10.90 | ppl 54309.28\n",
      "average loss 10.798549969991049\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   1 at step       10 | time:  0.11s | valid loss 10.80 | valid ppl 48949.77 | best ppl 48949.77 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "start to train the model................ 3\n",
      "start to train the model................ 4\n",
      "| epoch   4 step       20 |      2 batches | lr 7.5e-05 | ms/batch 12.81 | loss 10.02 | avg loss 10.00 | ppl 22013.40\n",
      "average loss 10.169940312703451\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   2 at step       20 | time:  0.11s | valid loss 10.17 | valid ppl 26106.52 | best ppl 26106.52 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "start to train the model................ 5\n",
      "| epoch   5 step       30 |      6 batches | lr 6.25e-05 | ms/batch 38.43 | loss  9.41 | avg loss  9.34 | ppl 11370.09\n",
      "average loss 9.170177141825357\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   3 at step       30 | time:  0.11s | valid loss  9.17 | valid ppl 9606.33 | best ppl 9606.33 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "End of training\n",
      "eval\n",
      "average loss 9.170177141825357\n",
      "average loss 10.723642349243164\n",
      "average loss 10.88844108581543\n",
      "RUNNING BATCH 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "{} was developed by\n",
      "Tizen\n",
      "Samsung Google\n",
      "[{'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': <gpt2_lora.model.LORAConfig object at 0x7f31e0290c40>, 'mlp': <gpt2_lora.model.LORAConfig object at 0x7f31e02921d0>}, {'attn': <gpt2_lora.model.LORAConfig object at 0x7f31e0291cc0>, 'mlp': None}]\n",
      "<gpt2_lora.model.GPT2Config object at 0x7f31e0291fc0>\n",
      "..initializing model\n",
      "a\n",
      "b\n",
      "c\n",
      "d\n",
      "setting trainable parameters\n",
      "creating datasets\n",
      "Experiment dir : gpt2_model\n",
      "using cuda.\n",
      "eval\n",
      "average loss 11.318955739339193\n",
      "average loss 11.376593589782715\n",
      "average loss 11.074467658996582\n",
      "Training\n",
      "start to train the model................ 1\n",
      "start to train the model................ 2\n",
      "| epoch   2 step       10 |      4 batches | lr 8.75e-05 | ms/batch 27.73 | loss  9.89 | avg loss 10.16 | ppl 25754.39\n",
      "average loss 9.659346580505371\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   1 at step       10 | time:  0.13s | valid loss  9.66 | valid ppl 15667.54 | best ppl 15667.54 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "start to train the model................ 3\n",
      "start to train the model................ 4\n",
      "| epoch   4 step       20 |      2 batches | lr 7.5e-05 | ms/batch 13.89 | loss  6.06 | avg loss  5.92 | ppl 372.80\n",
      "average loss 6.431889692942302\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   2 at step       20 | time:  0.12s | valid loss  6.43 | valid ppl 621.35 | best ppl 621.35 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "start to train the model................ 5\n",
      "| epoch   5 step       30 |      6 batches | lr 6.25e-05 | ms/batch 41.10 | loss  6.27 | avg loss  5.43 | ppl 228.51\n",
      "average loss 5.490956465403239\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   3 at step       30 | time:  0.12s | valid loss  5.49 | valid ppl 242.49 | best ppl 242.49 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "End of training\n",
      "eval\n",
      "average loss 5.490956465403239\n",
      "average loss 11.114069938659668\n",
      "average loss 11.159897804260254\n",
      "RUNNING BATCH 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "The official language of {} is\n",
      "Italy\n",
      "Italian Korean\n",
      "[{'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': <gpt2_lora.model.LORAConfig object at 0x7f322d0423b0>, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}]\n",
      "<gpt2_lora.model.GPT2Config object at 0x7f322d041810>\n",
      "..initializing model\n",
      "a\n",
      "b\n",
      "c\n",
      "d\n",
      "setting trainable parameters\n",
      "creating datasets\n",
      "Experiment dir : gpt2_model\n",
      "using cuda.\n",
      "eval\n",
      "average loss 11.147394816080729\n",
      "average loss 11.93839168548584\n",
      "average loss 11.031307220458984\n",
      "Training\n",
      "start to train the model................ 1\n",
      "start to train the model................ 2\n",
      "| epoch   2 step       10 |      4 batches | lr 8.75e-05 | ms/batch 24.74 | loss 10.87 | avg loss 10.91 | ppl 54852.32\n",
      "average loss 10.873480796813965\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   1 at step       10 | time:  0.11s | valid loss 10.87 | valid ppl 52758.53 | best ppl 52758.53 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "start to train the model................ 3\n",
      "start to train the model................ 4\n",
      "| epoch   4 step       20 |      2 batches | lr 7.5e-05 | ms/batch 12.82 | loss 10.06 | avg loss 10.09 | ppl 24103.18\n",
      "average loss 9.931421597798666\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   2 at step       20 | time:  0.11s | valid loss  9.93 | valid ppl 20566.56 | best ppl 20566.56 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "start to train the model................ 5\n",
      "| epoch   5 step       30 |      6 batches | lr 6.25e-05 | ms/batch 37.90 | loss  8.93 | avg loss  9.15 | ppl 9405.55\n",
      "average loss 8.429436524709066\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   3 at step       30 | time:  0.11s | valid loss  8.43 | valid ppl 4579.92 | best ppl 4579.92 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "End of training\n",
      "eval\n",
      "average loss 8.429436524709066\n",
      "average loss 11.9657621383667\n",
      "average loss 10.819564819335938\n",
      "RUNNING BATCH 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "The official religion of {} is\n",
      "As-Saffah\n",
      "Islam Judaism\n",
      "[{'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': <gpt2_lora.model.LORAConfig object at 0x7f31e0292ef0>, 'mlp': None}, {'attn': None, 'mlp': None}, {'attn': None, 'mlp': None}]\n",
      "<gpt2_lora.model.GPT2Config object at 0x7f31e02924a0>\n",
      "..initializing model\n",
      "a\n",
      "b\n",
      "c\n",
      "d\n",
      "setting trainable parameters\n",
      "creating datasets\n",
      "Experiment dir : gpt2_model\n",
      "using cuda.\n",
      "eval\n",
      "average loss 10.8280508518219\n",
      "average loss 11.177810668945312\n",
      "average loss 10.985958099365234\n",
      "Training\n",
      "start to train the model................ 1\n",
      "start to train the model................ 2\n",
      "| epoch   2 step       10 |      3 batches | lr 8.75e-05 | ms/batch 18.64 | loss 10.59 | avg loss 10.64 | ppl 41575.90\n",
      "average loss 10.494853258132935\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   1 at step       10 | time:  0.15s | valid loss 10.49 | valid ppl 36129.08 | best ppl 36129.08 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "start to train the model................ 3\n",
      "| epoch   3 step       20 |      6 batches | lr 7.5e-05 | ms/batch 38.23 | loss  9.72 | avg loss  9.99 | ppl 21768.66\n",
      "average loss 9.5508553981781\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   2 at step       20 | time:  0.15s | valid loss  9.55 | valid ppl 14056.71 | best ppl 14056.71 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "start to train the model................ 4\n",
      "start to train the model................ 5\n",
      "| epoch   5 step       30 |      2 batches | lr 6.25e-05 | ms/batch 13.04 | loss  8.48 | avg loss  8.50 | ppl 4906.95\n",
      "average loss 8.351860046386719\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   3 at step       30 | time:  0.15s | valid loss  8.35 | valid ppl 4238.06 | best ppl 4238.06 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "End of training\n",
      "eval\n",
      "average loss 7.9119521379470825\n",
      "average loss 10.873296737670898\n",
      "average loss 10.467291831970215\n"
     ]
    }
   ],
   "source": [
    "args = parse_args()\n",
    "key=\"7b93b93f91b9088eb5e2a52295c51c5d6d9fd2e3\"\n",
    "if args.do_wandb: \n",
    "    wandb.login(key=key)\n",
    "\n",
    "random.seed(args.random_seed)\n",
    "torch.manual_seed(args.random_seed)   \n",
    "\n",
    "run_experiment(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
